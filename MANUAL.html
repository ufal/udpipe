<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<meta name="generator" content="http://txt2tags.org">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>UDPipe</title>
</head>
<body>

<div class="header" id="header">
<h1>UDPipe</h1>
<h2>Version 1.2.1-devel</h2>
</div>

<div class="toc">

  <ol>
  <li><a href="#introduction">Introduction</a>
  </li>
  <li><a href="#online">Online Web Application and Web Service</a>
  </li>
  <li><a href="#release">Release</a>
    <ul>
    <li><a href="#download">3.1. Download</a>
      <ul>
      <li><a href="#language_models">3.1.1. Language Models</a>
      </li>
      </ul>
    </li>
    <li><a href="#license">3.2. License</a>
    </li>
    </ul>
  </li>
  <li><a href="#installation">UDPipe Installation</a>
    <ul>
    <li><a href="#requirements">4.1. Requirements</a>
    </li>
    <li><a href="#compilation">4.2. Compilation</a>
      <ul>
      <li><a href="#compilation_platforms">4.2.1. Platforms</a>
      </li>
      <li><a href="#compilation_further_details">4.2.2. Further Details</a>
      </li>
      </ul>
    </li>
    <li><a href="#other_language_bindings">4.3. Other language bindings</a>
      <ul>
      <li><a href="#csharp_installation">4.3.1. C#</a>
      </li>
      <li><a href="#java_installation">4.3.2. Java</a>
      </li>
      <li><a href="#perl_installation">4.3.3. Perl</a>
      </li>
      <li><a href="#python_installation">4.3.4. Python</a>
      </li>
      </ul>
    </li>
    </ul>
  </li>
  <li><a href="#users_manual">UDPipe User's Manual</a>
    <ul>
    <li><a href="#run_udpipe">5.1. Running UDPipe</a>
      <ul>
      <li><a href="#run_udpipe_immediate">5.1.1. Immediate Mode</a>
      </li>
      <li><a href="#run_udpipe_model_on_demand">5.1.2. Loading Model On Demand</a>
      </li>
      <li><a href="#run_udpipe_tokenizer">5.1.3. Tokenizer</a>
      </li>
      <li><a href="#run_udpipe_input">5.1.4. Input Formats</a>
      </li>
      <li><a href="#run_udpipe_tagger">5.1.5. Tagger</a>
      </li>
      <li><a href="#run_udpipe_parser">5.1.6. Dependency Parsing</a>
      </li>
      <li><a href="#run_udpipe_output">5.1.7. Output Formats</a>
      </li>
      </ul>
    </li>
    <li><a href="#udpipe_server">5.2. Running the UDPipe REST Server</a>
    </li>
    <li><a href="#model_training">5.3. Training UDPipe Models</a>
      <ul>
      <li><a href="#model_training_reusing_components">5.3.1. Reusing Components from Existing Models</a>
      </li>
      <li><a href="#model_training_random_search">5.3.2. Random Hyperparameter Search</a>
      </li>
      <li><a href="#model_training_tokenizer">5.3.3. Tokenizer</a>
      </li>
      <li><a href="#model_training_tagger">5.3.4. Tagger</a>
      </li>
      <li><a href="#model_training_parser">5.3.5. Parser</a>
      </li>
      <li><a href="#udpipe_accuracy">5.3.6. Measuring Model Accuracy</a>
      </li>
      </ul>
    </li>
    <li><a href="#universal_dependencies_20_models">5.4. Universal Dependencies 2.0 Models</a>
      <ul>
      <li><a href="#universal_dependencies_20_models_download">5.4.1. Download</a>
      </li>
      <li><a href="#universal_dependencies_20_models_acknowledgements">5.4.2. Acknowledgements</a>
      </li>
      <li><a href="#universal_dependencies_20_models_description">5.4.3. Model Description</a>
      </li>
      <li><a href="#universal_dependencies_20_models_performance">5.4.4. Model Performance</a>
      </li>
      </ul>
    </li>
    <li><a href="#conll17_shared_task_baseline_ud_20_models">5.5. CoNLL17 Shared Task Baseline UD 2.0 Models</a>
      <ul>
      <li><a href="#conll17_shared_task_baseline_ud_20_models_download">5.5.1. Download</a>
      </li>
      <li><a href="#conll17_shared_task_baseline_ud_20_models_ackowledgements">5.5.2. Acknowledgements</a>
      </li>
      </ul>
    </li>
    <li><a href="#universal_dependencies_12_models">5.6. Universal Dependencies 1.2 Models</a>
      <ul>
      <li><a href="#universal_dependencies_12_models_download">5.6.1. Download</a>
      </li>
      <li><a href="#universal_dependencies_12_models_acknowledgements">5.6.2. Acknowledgements</a>
      </li>
      <li><a href="#universal_dependencies_12_models_description">5.6.3. Model Description</a>
      </li>
      <li><a href="#universal_dependencies_12_models_performance">5.6.4. Model Performance</a>
      </li>
      </ul>
    </li>
    </ul>
  </li>
  <li><a href="#api_reference">UDPipe API Reference</a>
    <ul>
    <li><a href="#versioning">6.1. UDPipe Versioning</a>
    </li>
    <li><a href="#string_piece">6.2. Struct string_piece</a>
    </li>
    <li><a href="#token">6.3. Class token</a>
      <ul>
      <li><a href="#token_get_space_after">6.3.1. token::get_space_after()</a>
      </li>
      <li><a href="#token_set_space_after">6.3.2. token::set_space_after()</a>
      </li>
      <li><a href="#token_get_spaces_before">6.3.3. token::get_spaces_before()</a>
      </li>
      <li><a href="#token_set_spaces_before">6.3.4. token::set_spaces_before()</a>
      </li>
      <li><a href="#token_get_spaces_after">6.3.5. token::get_spaces_after()</a>
      </li>
      <li><a href="#token_set_spaces_after">6.3.6. token::set_spaces_after()</a>
      </li>
      <li><a href="#token_get_spaces_in_token">6.3.7. token::get_spaces_in_token()</a>
      </li>
      <li><a href="#token_set_spaces_in_token">6.3.8. token::set_spaces_in_token()</a>
      </li>
      <li><a href="#token_get_token_range">6.3.9. token::get_token_range()</a>
      </li>
      <li><a href="#token_set_token_range">6.3.10. token::set_token_range()</a>
      </li>
      </ul>
    </li>
    <li><a href="#word">6.4. Class word</a>
    </li>
    <li><a href="#multiword_token">6.5. Class multiword_token</a>
    </li>
    <li><a href="#empty_node">6.6. Class empty_node</a>
    </li>
    <li><a href="#sentence">6.7. Class sentence</a>
      <ul>
      <li><a href="#sentence_empty">6.7.1. sentence::empty()</a>
      </li>
      <li><a href="#sentence_clear">6.7.2. sentence::clear()</a>
      </li>
      <li><a href="#sentence_add_word">6.7.3. sentence::add_word()</a>
      </li>
      <li><a href="#sentence_set_head">6.7.4. sentence:set_head()</a>
      </li>
      <li><a href="#sentence_unlink_all_words">6.7.5. sentence::unlink_all_words()</a>
      </li>
      <li><a href="#sentence_get_new_doc">6.7.6. sentence::get_new_doc()</a>
      </li>
      <li><a href="#sentence_set_new_doc">6.7.7. sentence::set_new_doc()</a>
      </li>
      <li><a href="#sentence_get_new_par">6.7.8. sentence::get_new_par()</a>
      </li>
      <li><a href="#sentence_set_new_par">6.7.9. sentence::set_new_par()</a>
      </li>
      <li><a href="#sentence_get_sent_id">6.7.10. sentence::get_sent_id()</a>
      </li>
      <li><a href="#sentence_set_sent_id">6.7.11. sentence::set_sent_id()</a>
      </li>
      <li><a href="#sentence_get_text">6.7.12. sentence::get_text()</a>
      </li>
      <li><a href="#sentence_set_text">6.7.13. sentence::set_text()</a>
      </li>
      </ul>
    </li>
    <li><a href="#input_format">6.8. Class input_format</a>
      <ul>
      <li><a href="#input_format_read_block">6.8.1. input_format::read_block()</a>
      </li>
      <li><a href="#input_format_reset_document">6.8.2. input_format::reset_document()</a>
      </li>
      <li><a href="#input_format_set_text">6.8.3. input_format::set_text()</a>
      </li>
      <li><a href="#input_format_next_sentence">6.8.4. input_format::next_sentence()</a>
      </li>
      <li><a href="#input_format_new_input_format">6.8.5. input_format::new_input_format()</a>
      </li>
      <li><a href="#input_format_new_conllu_input_format">6.8.6. input_format::new_conllu_input_format()</a>
      </li>
      <li><a href="#input_format_new_generic_tokenizer_input_format">6.8.7. input_format::new_generic_tokenizer_input_format()</a>
      </li>
      <li><a href="#input_format_new_horizontal_input_format">6.8.8. input_format::new_horizontal_input_format()</a>
      </li>
      <li><a href="#input_format_new_vertical_input_format">6.8.9. input_format::new_vertical_input_format()</a>
      </li>
      <li><a href="#input_format_new_presegmented_tokenizer">6.8.10. input_format::new_presegmented_tokenizer()</a>
      </li>
      </ul>
    </li>
    <li><a href="#output_format">6.9. Class output_format</a>
      <ul>
      <li><a href="#output_format_write_sentence">6.9.1. output_format::write_sentence()</a>
      </li>
      <li><a href="#output_format_finish_document">6.9.2. output_format::finish_document()</a>
      </li>
      <li><a href="#output_format_new_output_format">6.9.3. output_format::new_output_format()</a>
      </li>
      <li><a href="#output_format_new_conllu_output_format">6.9.4. output_format::new_conllu_output_format()</a>
      </li>
      <li><a href="#output_format_new_epe_output_format">6.9.5. output_format::new_epe_output_format()</a>
      </li>
      <li><a href="#output_format_new_matxin_output_format">6.9.6. output_format::new_matxin_output_format()</a>
      </li>
      <li><a href="#output_format_new_plaintext_output_format">6.9.7. output_format::new_plaintext_output_format()</a>
      </li>
      <li><a href="#output_format_new_horizontal_output_format">6.9.8. output_format::new_horizontal_output_format()</a>
      </li>
      <li><a href="#output_format_new_vertical_output_format">6.9.9. output_format::new_vertical_output_format()</a>
      </li>
      </ul>
    </li>
    <li><a href="#model">6.10. Class model</a>
      <ul>
      <li><a href="#model_load_cstring">6.10.1. model::load(const char*)</a>
      </li>
      <li><a href="#model_load_istream">6.10.2. model::load(istream&amp;)</a>
      </li>
      <li><a href="#model_new_tokenizer">6.10.3. model::new_tokenizer()</a>
      </li>
      <li><a href="#model_tag">6.10.4. model::tag()</a>
      </li>
      <li><a href="#model_parse">6.10.5. model::parse()</a>
      </li>
      </ul>
    </li>
    <li><a href="#pipeline">6.11. Class pipeline</a>
      <ul>
      <li><a href="#pipeline_set_model">6.11.1. pipeline::set_model()</a>
      </li>
      <li><a href="#pipeline_set_input">6.11.2. pipeline::set_input()</a>
      </li>
      <li><a href="#pipeline_set_tagger">6.11.3. pipeline::set_tagger()</a>
      </li>
      <li><a href="#pipeline_set_parser">6.11.4. pipeline::set_parser()</a>
      </li>
      <li><a href="#pipeline_set_output">6.11.5. pipeline::set_output()</a>
      </li>
      <li><a href="#pipeline_set_immediate">6.11.6. pipeline::set_immediate()</a>
      </li>
      <li><a href="#pipeline_set_document_id">6.11.7. pipeline::set_document_id()</a>
      </li>
      <li><a href="#pipeline_process">6.11.8. pipeline::process()</a>
      </li>
      </ul>
    </li>
    <li><a href="#trainer">6.12. Class trainer</a>
      <ul>
      <li><a href="#trainer_train">6.12.1. trainer::train()</a>
      </li>
      </ul>
    </li>
    <li><a href="#evaluator">6.13. Class evaluator</a>
      <ul>
      <li><a href="#evaluator_set_model">6.13.1. evaluator::set_model()</a>
      </li>
      <li><a href="#evaluator_set_tokenizer">6.13.2. evaluator::set_tokenizer()</a>
      </li>
      <li><a href="#evaluator_set_tagger">6.13.3. evaluator::set_tagger()</a>
      </li>
      <li><a href="#evaluator_set_parser">6.13.4. evaluator::set_parser()</a>
      </li>
      <li><a href="#evaluator_evaluate">6.13.5. evaluator::evaluate()</a>
      </li>
      </ul>
    </li>
    <li><a href="#version">6.14. Class version</a>
      <ul>
      <li><a href="#version_current">6.14.1. version::current</a>
      </li>
      </ul>
    </li>
    <li><a href="#cpp_bindings_api">6.15. C++ Bindings API</a>
      <ul>
      <li><a href="#bindings_helper_structures">6.15.1. Helper Structures</a>
      </li>
      <li><a href="#bindings_main_classes">6.15.2. Main Classes</a>
      </li>
      </ul>
    </li>
    <li><a href="#csharp_bindings">6.16. C# Bindings</a>
    </li>
    <li><a href="#java_bindings">6.17. Java Bindings</a>
    </li>
    <li><a href="#perl_bindings">6.18. Perl Bindings</a>
    </li>
    <li><a href="#python_bindings">6.19. Python Bindings</a>
    </li>
    </ul>
  </li>
  <li><a href="#contact">Contact</a>
  </li>
  <li><a href="#udpipe_acknowledgements">Acknowledgements</a>
    <ul>
    <li><a href="#publications">8.1. Publications</a>
    </li>
    <li><a href="#bibtex_for_referencing">8.2. Bibtex for Referencing</a>
    </li>
    <li><a href="#persistent_identifier">8.3. Persistent Identifier</a>
    </li>
    </ul>
  </li>
  </ol>

</div>
<div class="body" ID="body">

<a id="introduction" name="introduction"></a>
<h1>1. Introduction</h1>

<p>
UDPipe is a trainable pipeline for tokenization, tagging, lemmatization and
dependency parsing of CoNLL-U files. UDPipe is language-agnostic and can be trained given
annotated data in <a href="http://universaldependencies.org/format.html">CoNLL-U format</a>. Trained models are provided for
nearly all <a href="http://universaldependencies.org">UD treebanks</a>. UDPipe is available as a binary for Linux/Windows/OS X, as a library for
C++, Python, Perl, Java, C#, and as a web service.
Externally, <a href="https://github.com/bnosac/udpipe">R wrapper</a> is also maintained.
</p>
<p>
UDPipe is a free software distributed under the
<a href="http://www.mozilla.org/MPL/2.0/">Mozilla Public License 2.0</a> and the linguistic models
are free for non-commercial use and distributed under the
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a> license, although for some
models the original data used to create the model may impose additional
licensing conditions. UDPipe is versioned using <a href="http://semver.org/">Semantic Versioning</a>.
</p>
<p>
Copyright 2017 by Institute of Formal and Applied Linguistics, Faculty of
Mathematics and Physics, Charles University, Czech Republic.
</p>

<a id="online" name="online"></a>
<h1>2. Online Web Application and Web Service</h1>

<p>
UDPipe Web Application is available at <a href="http://lindat.mff.cuni.cz/services/udpipe/">http://lindat.mff.cuni.cz/services/udpipe/</a>
using <a href="http://lindat.cz">LINDAT/CLARIN infrastructure</a>.
</p>
<p>
UDPipe REST Web Service is also available, with the API documentation available at
<a href="http://lindat.mff.cuni.cz/services/udpipe/api-reference.php">http://lindat.mff.cuni.cz/services/udpipe/api-reference.php</a>.
</p>

<a id="release" name="release"></a>
<h1>3. Release</h1>

<a id="download" name="download"></a>
<h2>3.1. Download</h2>

<p>
UDPipe releases are available on <a href="http://github.com/ufal/udpipe">GitHub</a>, both as
source code and as a pre-compiled binary package. The binary package contains Linux,
Windows and OS X binaries, Java bindings binary, C# bindings binary, and source
code of UDPipe and all language bindings). While the binary
packages do not contain compiled Python or Perl bindings, packages for those
languages are available in standard package repositories,
i.e. on <a href="https://pypi.python.org/pypi/ufal.udpipe/">PyPI</a>
and <a href="https://metacpan.org/pod/Ufal::UDPipe">CPAN</a>.
</p>

<ul>
<li><a href="http://github.com/ufal/udpipe/releases/latest">Latest release</a>
</li>
<li><a href="http://github.com/ufal/udpipe/releases">All releases</a>, <a href="https://github.com/ufal/udpipe/blob/master/CHANGES">Changelog</a>
</li>
</ul>

<a id="language_models" name="language_models"></a>
<h3>3.1.1. Language Models</h3>

<p>
To use UDPipe, a language model is needed. The language models are available
from <a href="http://www.lindat.cz">LINDAT/CLARIN</a> infrastructure and described further
in the
<a href="#users_manual">UDPipe User's Manual</a>.
Currently, the following language models are available:
</p>

<ul>
<li>Universal Dependencies 2.0 Models: <a href="http://hdl.handle.net/11234/1-2364">udpipe-ud2.0-170801</a> (<a href="http://ufal.mff.cuni.cz/udpipe/users-manual#universal_dependencies_20_models">documentation</a>)
</li>
<li>CoNLL17 Shared Task Baseline UD 2.0 Models: <a href="http://hdl.handle.net/11234/1-1990">udpipe-ud2.0-conll17-170315</a> (<a href="http://ufal.mff.cuni.cz/udpipe/users-manual#conll17_shared_task_baseline_ud_20_models">documentation</a>)
</li>
<li>Universal Dependencies 1.2 Models: <a href="http://hdl.handle.net/11234/1-1659">udpipe-ud1.2-160523</a> (<a href="http://ufal.mff.cuni.cz/udpipe/users-manual#universal_dependencies_12_models">documentation</a>)
</li>
</ul>

<a id="license" name="license"></a>
<h2>3.2. License</h2>

<p>
UDPipe is an open-source project and is freely available for non-commercial
purposes. The library is distributed under
<a href="http://www.mozilla.org/MPL/2.0/">Mozilla Public License 2.0</a> and the associated models and data
under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a>, although
for some models the original data used to create the model may impose
additional licensing conditions.
</p>
<p>
If you use this tool for scientific work, please give credit to us by
referencing <a href="#bibtex_for_referencing">Straka et al. 2016</a> and the
<a href="http://ufal.mff.cuni.cz/udpipe">UDPipe website</a>.
</p>

<a id="installation" name="installation"></a>
<h1>4. UDPipe Installation</h1>

<p>
UDPipe releases are available on <a href="http://github.com/ufal/udpipe">GitHub</a>, either as
a pre-compiled binary package, or source code only. The binary package contains Linux,
Windows and OS X binaries, Java bindings binary, C# bindings binary, and source
code of UDPipe and all language bindings. While the binary
packages do not contain compiled Python or Perl bindings, packages for those
languages are available in standard package repositories, i.e. on PyPI and CPAN.
</p>
<p>
To use UDPipe, a language model is needed.
<a href="http://ufal.mff.cuni.cz/udpipe#language_models">Here is a list of available language models</a>.
</p>
<p>
If you want to compile UDPipe manually, sources are available on on
<a href="http://github.com/ufal/udpipe">GitHub</a>, both in the
<a href="http://github.com/ufal/udpipe/releases">pre-compiled binary package releases</a>
and in the repository itself.
</p>

<a id="requirements" name="requirements"></a>
<h2>4.1. Requirements</h2>

<ul>
<li><code>g++ 4.7</code> or newer, <code>clang 3.2</code> or newer, Visual C++ 2015 or newer
</li>
<li><code>make</code>
</li>
<li><code>SWIG 3.0.8</code> or newer for language bindings other than <code>C++</code>
</li>
</ul>

<a id="compilation" name="compilation"></a>
<h2>4.2. Compilation</h2>

<p>
To compile UDPipe, run <code>make</code> in the <code>src</code> directory.
</p>
<p style="margin-bottom:0">
Make targets and options:
</p>
<ul style="margin-top:0">
<li><code>exe</code>: compile the binaries (default)
</li>
<li><code>server</code>: compile the REST server
</li>
<li><code>lib</code>: compile the static library
</li>
<li><code>BITS=32</code> or <code>BITS=64</code>: compile for specified 32-bit or 64-bit architecture instead of the default one
</li>
<li><code>MODE=release</code>: create release build which statically links the C++ runtime and uses LTO
</li>
<li><code>MODE=debug</code>: create debug build
</li>
<li><code>MODE=profile</code>: create profile build
</li>
</ul>

<a id="compilation_platforms" name="compilation_platforms"></a>
<h3>4.2.1. Platforms</h3>

<p style="margin-bottom:0">
Platform can be selected using one of the following options:
</p>
<ul style="margin-top:0">
<li><code>PLATFORM=linux</code>, <code>PLATFORM=linux-gcc</code>: gcc compiler on Linux operating system, default on Linux
</li>
<li><code>PLATFORM=linux-clang</code>: clang compiler on Linux, must be selected manually
</li>
<li><code>PLATFORM=osx</code>, <code>PLATFORM=osx-clang</code>: clang compiler on OS X, default on OS X; <code>BITS=32+64</code> enables multiarch build
</li>
<li><code>PLATFORM=win</code>, <code>PLATFORM=win-gcc</code>: gcc compiler on Windows (TDM-GCC is well tested), default on Windows
</li>
<li><code>PLATFORM=win-vs</code>: Visual C++ 2015 compiler on Windows, must be selected manually; note that the
  <code>cl.exe</code> compiler must be already present in <code>PATH</code> and corresponding <code>BITS=32</code> or <code>BITS=64</code>
  must be specified
</li>
</ul>

<p>
Either POSIX shell or Windows CMD can be used as shell, it is detected automatically.
</p>

<a id="compilation_further_details" name="compilation_further_details"></a>
<h3>4.2.2. Further Details</h3>

<p>
UDPipe uses <a href="http://github.com/ufal/cpp_builtem">C++ BuilTem system</a>,
please refer to its manual if interested in all supported options.
</p>

<a id="other_language_bindings" name="other_language_bindings"></a>
<h2>4.3. Other language bindings</h2>

<a id="csharp_installation" name="csharp_installation"></a>
<h3>4.3.1. C#</h3>

<p>
Binary C# bindings are available in UDPipe binary packages.
</p>
<p>
To compile C# bindings manually, run <code>make</code> in the <code>bindings/csharp</code>
directory, optionally with the options described in UDPipe Installation.
</p>

<a id="java_installation" name="java_installation"></a>
<h3>4.3.2. Java</h3>

<p>
Binary Java bindings are available in UDPipe binary packages.
</p>
<p>
To compile Java bindings manually, run <code>make</code> in the <code>bindings/java</code>
directory, optionally with the options described in UDPipe Installation.
Java 6 and newer is supported.
</p>
<p style="margin-bottom:0">
The Java installation specified in the environment variable <code>JAVA_HOME</code> is
used.  If the environment variable does not exist, the <code>JAVA_HOME</code> can be
specified using
</p>
<pre style="margin-top:0">
make JAVA_HOME=path_to_Java_installation
</pre>

<a id="perl_installation" name="perl_installation"></a>
<h3>4.3.3. Perl</h3>

<p>
The Perl bindings are available as <code>Ufal-UDPipe</code> package on CPAN.
</p>
<p>
To compile Perl bindings manually, run <code>make</code> in the <code>bindings/perl</code>
directory, optionally with the options described in UDPipe Installation.
Perl 5.10 and later is supported.
</p>
<p style="margin-bottom:0">
Path to the include headers of the required Perl version must be specified
in the <code>PERL_INCLUDE</code> variable using
</p>
<pre style="margin-top:0">
make PERL_INCLUDE=path_to_Perl_includes
</pre>

<a id="python_installation" name="python_installation"></a>
<h3>4.3.4. Python</h3>

<p>
The Python bindings are available as <code>ufal.udpipe</code> package on PyPI.
</p>
<p>
To compile Python bindings manually, run <code>make</code> in the <code>bindings/python</code>
directory, optionally with options described in UDPipe Installation. Both
Python 2.6+ and Python 3+ are supported.
</p>
<p style="margin-bottom:0">
Path to the include headers of the required Python version must be specified
in the <code>PYTHON_INCLUDE</code> variable using
</p>
<pre style="margin-top:0">
make PYTHON_INCLUDE=path_to_Python_includes
</pre>

<a id="users_manual" name="users_manual"></a>
<h1>5. UDPipe User's Manual</h1>

<p>
Like any supervised machine-learning tool, UDPipe needs a trained linguistic model.
This section describes the available language models and also the command line
tools and interfaces.
</p>

<a id="run_udpipe" name="run_udpipe"></a>
<h2>5.1. Running UDPipe</h2>

<p style="margin-bottom:0">
Probably the most common usage of UDPipe is to tokenize, tag and parse the input using
</p>
<pre style="margin-top:0">
udpipe --tokenize --tag --parse udpipe_model
</pre>

<p>
The input is assumed to be in UTF-8 encoding and can be either already tokenized
and segmented, or it can be a plain text which will be tokenized and segmented automatically.
</p>
<p>
Any number of input files can be specified after the <code>udpipe_model</code> and if no
file is given, the standard input is used. The output is by default
saved to the standard output, but if <code>--outfile=name</code> is used, it is saved
to the given file name. The output file name can contain a <code>{}</code>, which is
replaced by a base name of the processed file (i.e., without directories
and an extension).
</p>
<p style="margin-bottom:0">
The full command syntax of running UDPipe is
</p>
<pre style="margin-top:0">
Usage: udpipe [running_opts] udpipe_model [input_files]
       udpipe --train [training_opts] udpipe_model [input_files]
       udpipe --detokenize [detokenize_opts] raw_text_file [input_files]
Running opts: --accuracy (measure accuracy only)
              --input=[conllu|generic_tokenizer|horizontal|vertical]
              --immediate (process sentences immediately during loading)
              --outfile=output file template
              --output=[conllu|matxin|horizontal|plaintext|vertical]
              --tokenize (perform tokenization)
              --tokenizer=tokenizer options, implies --tokenize
              --tag (perform tagging)
              --tagger=tagger options, implies --tag
              --parse (perform parsing)
              --parser=parser options, implies --parse
Training opts: --method=[morphodita_parsito] which method to use
               --heldout=heldout data file name
               --tokenizer=tokenizer options
               --tagger=tagger options
               --parser=parser options
Detokenize opts: --outfile=output file template
Generic opts: --version
              --help
</pre>

<a id="run_udpipe_immediate" name="run_udpipe_immediate"></a>
<h3>5.1.1. Immediate Mode</h3>

<p>
By default UDPipe loads the whole input file into memory before starting
to process it. That allows to store the space markup (see the following Tokenizer
section) in most consistent way, i.e., store all spaces following a sentence
in the last token of that sentence.
</p>
<p>
However, sometimes it is desirable to process the input as soon as possible,
which can be achieved by specifying the <code>--immediate</code> option. In immediate mode,
the input is processed and printed as soon as a block of input guaranteed to contain
whole sentences is loaded. Specifically, for most input formats the input is
processed after loading an empty line (with the exception of <code>horizontal</code>
input format and <code>presegmented</code> tokenizer, where the input is processed after
each line).
</p>

<a id="run_udpipe_model_on_demand" name="run_udpipe_model_on_demand"></a>
<h3>5.1.2. Loading Model On Demand</h3>

<p>
Although a model for UDPipe always has to be specified, the model is loaded
only if really needed. It is therefore possible to use for example <code>none</code>
as the model in case it is not required for performing the requested
operation (e.g., converting between formats or using a generic tokenizer).
</p>

<a id="run_udpipe_tokenizer" name="run_udpipe_tokenizer"></a>
<h3>5.1.3. Tokenizer</h3>

<p style="margin-bottom:0">
If the <code>--tokenize</code> option is supplied, the input is assumed to be
plain text and is tokenized using model tokenizer. Additional arguments to the
tokenizer might be specified using <code>--tokenizer=data</code> option (which implies
<code>--tokenize</code>), where <code>data</code> is a semicolon-separated list of the following options:
</p>
<ul style="margin-top:0">
<li><code>normalized_spaces</code>: by default, UDPipe uses custom MISC fields to exactly
  encode spaces in the original document (as described below). If the
  <code>normalized_spaces</code> option is given, only the standard CoNLL-U v2 markup
  (<code>SpaceAfter=No</code> and <code># newpar</code>) is used.
</li>
<li><code>presegmented</code>: the input file is assumed to be already segmented, with each
  sentence on a separate line, and is only tokenized (respecting sentence breaks)
</li>
<li><code>ranges</code>: for each token, a range in the original document is stored in the
  format described below.
</li>
<li><code>joint_with_parsing</code>: an experimental mode performing sentence segmentation
  jointly using the tokenizer and the parser (see <i><a href="http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf">Milan Straka and Jana Strakov√°: Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe</a></i> paper for details).
  The following options are utilized:
  <ul>
  <li><code>joint_max_sentence_len</code> (default 20): maximum sentence length
  </li>
  <li><code>joint_change_boundary_logprob</code> (default -0.5): logprob of using sentence boundary not generated by the tokenizer
  </li>
  <li><code>joint_sentence_logprob</code> (default -0.5): additional logprob of every sentence
  </li>
  </ul>
  The logprob of a sentence is computed using logprob of its best dependency parsing tree,
  together with <code>joint_sentence_logprob</code> and also <code>joint_change_boundary_logprob</code>
  for every sentence boundary not returned by the tokenizer (i.e., either 0, 1 or 2 times).
  The joint sentence segmentation chooses such a segmentation, where every
  sentence has length at most <code>joint_max_sentence_len</code> and the sum of logprobs of all sentences
  is as large as possible.
</li>
</ul>

<a id="run_udpipe_tokenizer_spaces" name="run_udpipe_tokenizer_spaces"></a>
<h4>5.1.3.1. Preserving Original Spaces</h4>

<p>
By default, UDPipe uses custom MISC fields to store all spaces in the original
document. This markup is backward compatible with CoNLL-U v2 <code>SpaceAfter=No</code> feature.
This markup can be utilized by the <code>plaintext</code> output format, which allows reconstructing
the original document.
</p>
<p>
Note that in theory not only spaces, but also other original content can be saved in this
way (for example XML tags if the input was encoded in a XML file).
</p>
<p style="margin-bottom:0">
The markup uses the following MISC fields on <i>tokens</i> (not words in multi-word tokens):
</p>
<ul style="margin-top:0">
<li><code>SpacesBefore=content</code> (by default empty): spaces/other content preceding the token
</li>
<li><code>SpacesAfter=content</code> (by default a space if <code>SpaceAfter=No</code> feature is not present, empty otherwise):
  spaces/other content following the token
</li>
<li><code>SpacesInToken=content</code> (by default equal to the FORM of the token): FORM of the token
  including original spaces (this is needed only if tokens are allowed to contain spaces and
  a token contains a tab or newline characters)
</li>
</ul>

<p style="margin-bottom:0">
The <code>content</code> of all the three fields must be escaped to allow storing tabs and newlines.
The following C-like schema is used:
</p>
<ul style="margin-top:0">
<li><code>\s</code>: space
</li>
<li><code>\t</code>: tab
</li>
<li><code>\r</code>: CR character
</li>
<li><code>\n</code>: LF character
</li>
<li><code>\p</code>: | (pipe character)
</li>
<li><code>\\</code>: \ (backslash character)
</li>
</ul>

<a id="run_udpipe_tokenizer_ranges" name="run_udpipe_tokenizer_ranges"></a>
<h4>5.1.3.2. Preserving Token Ranges</h4>

<p>
When the <code>ranges</code> tokenizer option is used, the range of each token in the original document
is stored in the <code>TokenRange</code> MISC field.
</p>
<p>
The format of the <code>TokenRange</code> field (inspired by Python) is <code>TokenRange=start:end</code>,
where <code>start</code> is a zero-based document-level index of the start of the token
(counted in Unicode characters) and <code>end</code> is a zero-based document-level index
of the first character following the token (i.e., the length of the token is <code>end-start</code>).
</p>

<a id="run_udpipe_input" name="run_udpipe_input"></a>
<h3>5.1.4. Input Formats</h3>

<p style="margin-bottom:0">
If the tokenizer is not used, the input format can be specified using the
<code>--input</code> option. The individual input formats can be parametrized in the
same way a tokenizer is, by using <code>format=data</code> syntax. Currently supported
input formats are:
</p>
<ul style="margin-top:0">
<li><code>conllu</code> (default): the <a href="http://universaldependencies.org/docs/format.html">CoNLL-U format</a>.
  Supported options:
  <ul>
  <li><code>v2</code> (default): use CoNLL-U v2
  </li>
  <li><code>v1</code>: allow loading only CoNLL-U v1 (i.e., no empty nodes and no spaces in forms and lemmas)
  </li>
  </ul>
</li>
<li><code>generic_tokenizer</code>: generic tokenizer for English-like languages (with spaces separating tokens
  and English-like punctuation). The tokenizer is rule-based and needs no trained model.
  It supports the same options as a model tokenizer, i.e., <code>normalized_spaces</code>, <code>presegmented</code>
  and <code>ranges</code>.
</li>
<li><code>horizontal</code>: each sentence on a separate line, with tokens separated by spaces. In order to allow
  spaces in tokens, Unicode character 'NO-BREAK SPACE' (U+00A0) is considered part of token
  and converted to a space during loading.
</li>
<li><code>vertical</code>: each token on a separate line, with an empty line denoting end of sentence;
  only the first tab-separated word is used as a token, the rest of the line is ignored.
</li>
</ul>

<p>
Note that a model tokenizer can be specified using the <code>--input</code> option too, by
using the <code>tokenizer</code> input format, for example using <code>--input tokenizer=ranges</code>.
</p>

<a id="run_udpipe_tagger" name="run_udpipe_tagger"></a>
<h3>5.1.5. Tagger</h3>

<p>
If the <code>--tag</code> option is supplied, the input is POS tagged and lemmatized
using the model tagger. Additional arguments to the tagger might be
specified using the <code>--tagger=data</code> option (which implies <code>--tag</code>).
</p>

<a id="run_udpipe_parser" name="run_udpipe_parser"></a>
<h3>5.1.6. Dependency Parsing</h3>

<p>
If the <code>--parse</code> option is supplied, the input is parsed using
the model dependency parser.  Additional arguments to the parser might be
specified using the <code>--parser=data</code> option (which implies <code>--parse</code>).
</p>

<a id="run_udpipe_output" name="run_udpipe_output"></a>
<h3>5.1.7. Output Formats</h3>

<p style="margin-bottom:0">
The output format is specified using the <code>--output</code> option. The individual
output formats can be parametrized in the same way as input formats, by using the
<code>format=data</code> syntax. Currently supported output formats are:
</p>
<ul style="margin-top:0">
<li><code>conllu</code> (default): the <a href="http://universaldependencies.org/docs/format.html">CoNLL-U format</a>
  Supported options:
  <ul>
  <li><code>v2</code> (default): use CoNLL-U v2
  </li>
  <li><code>v1</code>: produce output in CoNLL-U v1 format. Note that this is a lossy process, as
  empty nodes are ignored and spaces in forms and lemmas are converted to underscores.
  </li>
  </ul>
</li>
<li><code>matxin</code>: the <a href="http://wiki.apertium.org/wiki/Matxin">Matxin format</a>
</li>
<li><code>horizontal</code>: writes the <i>words</i> (in the UD sense) in horizontal format,
  that is, each sentence is on a separate line, with words separated by
  a single space. Because words can contain spaces in CoNLL-U v2, the spaces in
  words are converted to Unicode character 'NO-BREAK SPACE' (U+00A0).
  Supported options:
  <ul>
  <li><code>paragraphs</code>: an empty line is printed after the end of a paragraph
  or a document (recognized by <code># newpar</code> or <code># newdoc</code> comments)
  </li>
  </ul>
</li>
<li><code>plaintext</code>: writes the <i>tokens</i> (in the UD sense) using original spacing.
  By default, UDPipe's custom MISC features (<code>SpacesBefore</code>, <code>SpacesAfter</code> and <code>SpacesInToken</code>,
  see the description in the Tokenizer section) are used to reconstruct the exact original spaces.
  However, if the document does not contain these features or if you want only normalized
  spacing, you can use the following option:
  <ul>
  <li><code>normalized_spaces</code>: write one sentence on a line, and either one or no space between
  tokens according to the <code>SpaceAfter=No</code> feature
  </li>
  </ul>
</li>
<li><code>vertical</code>: each word on a separate line, with an empty line denoting the end of sentence.
  Supported options:
  <ul>
  <li><code>paragraphs</code>: an empty line is printed after the end of a paragraph
  or a document (recognized by <code># newpar</code> or <code># newdoc</code> comments)
  </li>
  </ul>
</li>
</ul>

<a id="udpipe_server" name="udpipe_server"></a>
<h2>5.2. Running the UDPipe REST Server</h2>

<p>
UDPipe also provides a REST server binary called <code>udpipe_server</code>.
The binary uses <a href="http://github.com/ufal/microrestd">MicroRestD</a> as a REST
server implementation and provides
<a href="http://lindat.mff.cuni.cz/services/udpipe/api-reference.php">UDPipe REST API</a>.
</p>
<p style="margin-bottom:0">
The full command syntax of <code>udpipe_server</code> is
</p>
<pre style="margin-top:0">
udpipe_server [options] port default_model (rest_id model_file acknowledgements)*
Options: --concurrent_models=maximum concurrently loaded models (default 10)
         --daemon (daemonize after start)
         --no_check_models_loadable (do not check models are loadable)
         --no_preload_default (do not preload default model)
</pre>

<p>
The <code>udpipe_server</code> can run either in foreground or in background (when
<code>--daemon</code> is used).
</p>
<p>
Since UDPipe 1.1.1, the models are loaded on demand, so that at most
<code>concurrent_models</code> (default 10) are kept in memory at the same time.
The model files are opened during start and never closed until the server
stops. Unless <code>no_check_models_loadable</code> is specified, the model files
are also checked to be loadable during start. Note that the default model
is preloaded and never released, unless <code>no_preload_default</code> is given.
(Before UDPipe 1.1.1, specified model files were loaded during start and
kept in memory all the time.)
</p>

<a id="model_training" name="model_training"></a>
<h2>5.3. Training UDPipe Models</h2>

<p style="margin-bottom:0">
Custom UDPipe models can be trained using the following syntax:
</p>
<pre style="margin-top:0">
udpipe --train model.output [--heldout=heldout_data] training_file ...
</pre>

<p>
The training data should be in the <a href="http://universaldependencies.org/docs/format.html">CoNLL-U format</a>.
</p>
<p>
By default, three model components are trained &ndash; tokenizer, tagger and
parser.  Any subset of the model components can be trained and a model
component may be copied from an existing model.
</p>
<p>
The training options are specified for each model component separately
using the <code>--tokenizer</code>, <code>--tagger</code> and <code>--parser</code> options.
If a model component should not be trained, value <code>none</code> should
be used (e.g., <code>--tagger=none</code>).
</p>
<p>
The options are <code>name=value</code> pairs separated by a semicolon. The
value can be either a simple string value (ending by a semicolon),
file content specified as <code>name=file:filename</code>, or an arbitrary
string value specified as <code>name=data:length:value</code>, where the value
is exactly <code>length</code> bytes long.
</p>

<a id="model_training_reusing_components" name="model_training_reusing_components"></a>
<h3>5.3.1. Reusing Components from Existing Models</h3>

<p>
The model components (tagger, parser or tagger) can be reused from
existing models, by specifying the <code>from_model=file:filename</code> option.
</p>

<a id="model_training_random_search" name="model_training_random_search"></a>
<h3>5.3.2. Random Hyperparameter Search</h3>

<p>
The default values of hyperparameters are set to the values
which were used the most during UD 1.2 models training, but if you
want to reach best performance, the hyperparameters must be tuned.
</p>
<p>
Apart from manual grid search, UDPipe can perform a simple random search.
You can perform the random search by repeatedly training UDPipe
(preferably in parallel, most likely on different computers)
while specifying different training run number &ndash; some of the
hyperparameters (chosen by us; you can of course override their value
by specifying it on the command line) change their values in different
training runs. The pseudorandom sequences of hyperparameters
are of course deterministic.
</p>
<p>
The training run can be specified by providing the <code>run=number</code> option
to a model component. The run number 1 is the default one (with the best
hyperparameters for the UD 1.2 models), run numbers 2 and more randomize
the hyperparameters.
</p>

<a id="model_training_tokenizer" name="model_training_tokenizer"></a>
<h3>5.3.3. Tokenizer</h3>

<p>
The tokenizer is trained using the <code>SpaceAfter=No</code> features in the CoNLL-U files.
If the feature is not present, a <i>detokenizer</i> can be used to guess
the <code>SpaceAfter=No</code> features according to a supplied plain text
(which typically does not overlap with the texts in the CoNLL-U files).
</p>
<p>
In order to use the detokenizer, use the <code>detokenizer=file:filename_with_plaintext</code>
option. In UD 1.2 models, the optimal performance is achieved with very small plain
texts &ndash; only 500kB.
</p>
<p style="margin-bottom:0">
The tokenizer recognizes the following options:
</p>
<ul style="margin-top:0">
<li><code>tokenize_url</code> (default 1): tokenize URLs and emails using a manually
    implemented recognizer
</li>
<li><code>allow_spaces</code> (default 1 if any token contains a space, 0 otherwise): allow tokens to contain spaces
</li>
<li><code>dimension</code> (default 24): dimension of character embeddings and of the per-character bidirectional
    GRU. Note that inference time is quadratic in this parameter. Supported values
    are only 16, 24 and 64, with 64 needed only for languages with complicated tokenization
    like Japanese, Chinese or Vietnamese.
</li>
<li><code>epochs</code> (default 100): the number of epochs to train the tokenizer for
</li>
<li><code>batch_size</code> (default 50): batch size used during tokenizer training
</li>
<li><code>learning_rate</code> (default 0.005): the learning rate used during tokenizer training
</li>
<li><code>dropout</code> (default 0.1): dropout used during tokenizer training
</li>
<li><code>early_stopping</code> (default 1 if heldout is given, 0 otherwise): perform
    early stopping, choosing training iteration maximizing sentences F1 score plus
    tokens F1 score on heldout data
</li>
</ul>

<p>
During random hyperparameter search, <code>batch_size</code> is chosen uniformly from <i>{50,100}</i>
and <code>learning_rate</code> logarithmically from <i>&lt;0.0005, 0.01)</i>.
</p>

<a id="model_training_detokenizer" name="model_training_detokenizer"></a>
<h4>5.3.3.1. Detokenizing CoNLL-U Files</h4>

<p>
The <code>--detokenizer</code> option allows generating the <code>SpaceAfter=No</code> features
automatically from a given plain text. Even if the current algorithm is very
simple and makes quite a lot of mistakes, the tokenizer trained on generated
features is very close to a tokenizer trained on gold <code>SpaceAfter=No</code>
features (the difference in token F1 score is usually one or two tenths of percent).
</p>
<p>
The generated <code>SpaceAfter=No</code> features are only used during tokenizer
training, not printed. However, if you would like to obtain the CoNLL-U files
with automatic detokenization (generated <code>SpaceAfter=No</code> features), you can
run UDPipe with the <code>--detokenize</code> option. In this case, you have to supply
plain text in the given language (usually the best results are achieved with
just 500kB or 1MB of text) and UDPipe then detokenizes all the given CoNLL-U files.
</p>
<p style="margin-bottom:0">
The complete usage of the <code>--detokenize</code> option is:
</p>
<pre style="margin-top:0">
udpipe --detokenize [detokenize_opts] raw_text_file [input_files]
Detokenize opts: --outfile=output file template
</pre>

<a id="model_training_tagger" name="model_training_tagger"></a>
<h3>5.3.4. Tagger</h3>

<p>
The tagging is currently performed using <a href="http://ufal.mff.cuni.cz/morphodita">MorphoDiTa</a>.
The UDPipe tagger consists of possibly several MorphoDiTa models, each tagging
some of the POS tags and/or lemmas.
</p>
<p>
By default, only one model is constructed, which generates all available tags (UPOS,
XPOS, Feats and Lemma). However, we found out during the UD 1.2 models training
that performance improves if one model tags the UPOS, XPOS and Feats tags, while the
other is performing lemmatization. Therefore, if you utilize two MorphoDiTa models,
by default the first one generates all tags (except lemmas) and the second one performs lemmatization.
</p>
<p>
The number of MorphoDiTa models can be specified using the <code>models=number</code> parameter.
All other parameters may be either generic for all models (<code>guesser_suffix_rules=5</code>),
or specific for a given model (<code>guesser_suffix_rules_2=6</code>), including the
<code>from_model</code> option (therefore, MorphoDiTa models can be trained separately
and then combined together into one UDPipe model).
</p>
<p>
Every model utilizes UPOS for disambiguation and the first model is the one producing
the UPOS tags on output.
</p>
<p style="margin-bottom:0">
The tagger recognizes the following options:
</p>
<ul style="margin-top:0">
<li><code>use_lemma</code> (default for the second model and also if there is only one model): use the
  lemma field internally to perform disambiguation; the lemma may be not outputted
</li>
<li><code>provide_lemma</code> (default for the second model and also if there is only one model):
  produce the disambiguated lemma on output
</li>
<li><code>use_xpostag</code> (default for the first model): use the
  XPOS tags internally to perform disambiguation; it may not be outputted
</li>
<li><code>provide_xpostag</code> (default for the first model): produce the disambiguated XPOS tag on output
</li>
<li><code>use_feats</code> (default for the first model): use the
  Feats internally to perform disambiguation; it may not be outputted
</li>
<li><code>provide_feats</code> (default for the first model): produce the disambiguated Feats field on output
</li>
<li><code>dictionary_max_form_analyses</code> (default 0 - unlimited): the maximum number of
  (most frequent) form analyses from UD training data that are to be
  kept in the morphological dictionary
</li>
<li><code>dictionary_file</code> (default empty): use a given custom morphological dictionary, where each line contains
  5 tab-separated fields FORM, LEMMA, UPOSTAG, XPOSTAG and FEATS. Note that this dictionary
  data is appended to the dictionary created from the UD training data, not replacing it.
</li>
<li><code>guesser_suffix_rules</code> (default 8): number of rules generated for every suffix
</li>
<li><code>guesser_prefixes_max</code> (default 4 if ``provide_lemma`, 0 otherwise): maximum
  number of form-generating prefixes to use in the guesser
</li>
<li><code>guesser_prefix_min_count</code> (default 10): minimum number of occurrences of form-generating
  prefix to consider using it in the guesser
</li>
<li><code>guesser_enrich_dictionary</code> (default 6 if no <code>dictionary_file</code> is passed, 0 otherwise): number of rules generated for forms present
  in training data (assuming that the analyses from the training data may not be all)
</li>
<li><code>iterations</code> (default 20): number of training iterations to perform
</li>
<li><code>early_stopping</code> (default 1 if heldout is given, 0 otherwise): perform
    early stopping, choosing training iteration maximizing tagging accuracy on the heldout data
</li>
<li><code>templates</code> (default <code>lemmatizer</code> for second model, <code>tagger</code> otherwise): MorphoDiTa
    feature templates to use, either <code>lemmatizer</code> which focuses more on lemmas, or
    <code>tagger</code> which focuses more on UPOS/XPOS/FEATS
</li>
</ul>

<p>
During random hyperparameter search, <code>guesser_suffix_rules</code> is chosen uniformly from
<i>{5,6,7,8,9,10,11,12}</i> and <code>guesser_enrich_dictionary</code> is chosen uniformly from
<i>{3,4,5,6,7,8,9,10}</i>.
</p>

<a id="model_training_parser" name="model_training_parser"></a>
<h3>5.3.5. Parser</h3>

<p>
The parsing is performed using <a href="http://ufal.mff.cuni.cz/parsito">Parsito</a>, which is a
transition-based parser using a neural-network classifier.
</p>
<p style="margin-bottom:0">
The transition-based systems can be configured by the following options:
</p>
<ul style="margin-top:0">
<li><code>transition_system</code> (default projective): which transition system to use for parsing (language dependent, you can choose according
  to language properties or try all and choose the best one)
  <ul>
  <li><code>projective</code>: projective stack-based arc standard system with <code>shift</code>, <code>left_arc</code> and <code>right_arc</code> transitions
  </li>
  <li><code>swap</code>: fully non-projective system which extends <code>projective</code> system by adding the <code>swap</code> transition
  </li>
  <li><code>link2</code>: partially non-projective system which extends <code>projective</code> system by adding <code>left_arc2</code> and <code>right_arc2</code> transitions
  </li>
  </ul>
</li>
<li><code>transition_oracle</code> (default dynamic/static_lazy_static whichever first is applicable): which transition oracle to use for the chosen <code>transition_system</code>:
  <ul>
  <li><code>transition_system=projective</code>: available oracles are <code>static</code> and <code>dynamic</code> (<code>dynamic</code> usually gives better results, but training time is slower)
  </li>
  <li><code>transition_system=swap</code>: available oracles are <code>static_eager</code> and <code>static_lazy</code> (<code>static_lazy</code> almost always gives better results)
  </li>
  <li><code>transition_system=link2</code>: only available oracle is <code>static</code>
  </li>
  </ul>
</li>
<li><code>structured_interval</code> (default 8): use search-based oracle in addition to the <code>translation_oracle</code> specified. This almost always gives better results, but makes training 2-3 times slower. For details, see the paper <i>Straka et al. 2015: Parsing Universal Dependency Treebanks using Neural Networks and Search-Based Oracle</i>
</li>
<li><code>single_root</code> (default 1): allow only single root when parsing, and make sure only the root node has the <code>root</code> deprel (note that training data are checked to be in this format)
</li>
</ul>

<p style="margin-bottom:0">
The Lemmas/UPOS/XPOS/FEATS used by the parser are configured by:
</p>
<ul style="margin-top:0">
<li><code>use_gold_tags</code> (default 0): if false and a tagger exists, the
  Lemmas/UPOS/XPOS/FEATS for both the training and heldout data are generated
  by the tagger, otherwise they are taken from the gold data
</li>
</ul>

<p style="margin-bottom:0">
The embeddings used by the parser can be specified as follows:
</p>
<ul style="margin-top:0">
<li><code>embedding_upostag</code> (default 20): the dimension of the UPos embedding used in the parser
</li>
<li><code>embedding_feats</code> (default 20): the dimension of the Feats embedding used in the parser
</li>
<li><code>embedding_xpostag</code> (default 0): the dimension of the XPos embedding used in the parser
</li>
<li><code>embedding_form</code> (default 50): the dimension of the Form embedding used in the parser
</li>
<li><code>embedding_lemma</code> (default 0): the dimension of the Lemma embedding used in the parser
</li>
<li><code>embedding_deprel</code> (default 20): the dimension of the Deprel embedding used in the parser
</li>
<li><code>embedding_form_file</code>: pre-trained word embeddings in <code>word2vec</code> textual format
</li>
<li><code>embedding_lemma_file</code>: pre-trained lemma embeddings in <code>word2vec</code> textual format
</li>
<li><code>embedding_form_mincount</code> (default 2): for forms not present in the pre-trained embeddings,
  generate random embeddings if the form appears at least this number of times in the
  trainig data (forms not present in the pre-trained embeddings and appearing less number
  of times are considered OOV)
</li>
<li><code>embedding_lemma_mincount</code> (default 2): for lemmas not present in the pre-trained embeddings,
  generate random embeddings if the lemma appears at least this number of times in the
  trainig data (lemmas not present in the pre-trained embeddings and appearing less number
  of times are considered OOV)
</li>
</ul>

<p style="margin-bottom:0">
The neural-network training options:
</p>
<ul style="margin-top:0">
<li><code>iterations</code> (default 10): number of training iterations to use
</li>
<li><code>hidden_layer</code> (default 200): the size of the hidden layer
</li>
<li><code>batch_size</code> (default 10): batch size used during neural-network training
</li>
<li><code>learning_rate</code> (default 0.02): the learning rate used during neural-network training
</li>
<li><code>learning_rate_final</code> (0.001): the final learning rate used during neural-network training
</li>
<li><code>l2</code> (0.5): the L2 regularization used during neural-network training
</li>
<li><code>early_stopping</code> (default 1 if heldout is given, 0 otherwise): perform
    early stopping, choosing training iteration maximizing LAS on heldout data
</li>
</ul>

<p>
During random hyperparameter search, <code>structured_interval</code> is chosen uniformly from
<i>{0,8,10}</i>, <code>learning_rate</code> is chosen logarithmically from <code>&lt;0.005,0.04)</code> and
<code>l2</code> is chosen uniformly from <code>&lt;0.2,0.6)</code>.
</p>

<a id="udpipe_training_parser_embeddings" name="udpipe_training_parser_embeddings"></a>
<h4>5.3.5.1. Pre-trained Word Embeddings</h4>

<p>
The pre-trained word embeddings for forms and lemmas can be specified in the
<code>word2vec</code> textual format using the <code>embedding_form_file</code> and
<code>embedding_lemma_file</code> options.
</p>
<p>
Note that pre-training word embeddings even on the UD data itself improves the
accuracy (we use <code>word2vec</code> with
<code>-cbow 0 -size 50 -window 10 -negative 5 -hs 0 -sample 1e-1 -threads 12 -binary 0 -iter 15 -min-count 2</code>
options to pre-train on the UD data after converting it to the horizontal format using
<code>udpipe --output=horizontal</code>).
</p>
<p>
Forms and lemmas can contain spaces in CoNLL-U v2, so these spaces are
converted to a Unicode character 'NO-BREAK SPACE' (U+00A0) before performing the
embedding lookup, because spaces are usually used to delimit tokens in word
embedding generating software (both <code>word2vec</code> and <code>glove</code> use spaces to
separate words on input and on output). When using UDPipe to generate
plain texts from CoNLL-U format using <code>--output=horizontal</code>, this space
replacing happens automatically.
</p>
<p style="margin-bottom:0">
When looking up an embedding for a given word, the following possibilities are
tried in the following order until a match is found (or an embedding for unknown word is returned):
</p>
<ul style="margin-top:0">
<li>original word
</li>
<li>all but the first character lowercased
</li>
<li>all characters lowercased
</li>
<li>if the word contains only digits, just the first digit is tried
</li>
</ul>

<a id="udpipe_accuracy" name="udpipe_accuracy"></a>
<h3>5.3.6. Measuring Model Accuracy</h3>

<p style="margin-bottom:0">
Measuring custom model accuracy can be performed by running:
</p>
<pre style="margin-top:0">
udpipe --accuracy [udpipe_options] udpipe_model file ...
</pre>

<p>
The command syntax is similar to the regular UDPipe operation, only
the input must be always in the <a href="http://universaldependencies.org/docs/format.html">CoNLL-U format</a>
and the <code>--input</code> and <code>--output</code> options are ignored.
</p>
<p style="margin-bottom:0">
Three different settings (depending on <code>--tokenize(r)</code>, <code>--tag(ger)</code> and <code>--parse(r)</code>)
can be evaluated:
</p>
<ul style="margin-top:0">
<li><code>--tokenize(r) [--tag(ger) [--parse(r)]]</code>: Tokenizer is used to segment
  and tokenize plain text (obtained by <code>SpaceAfter=No</code> features and <code># newdoc</code>
  and <code># newpar</code> comments in the input file). Optionally, a tagger is used on the
  resulting data to obtain Lemma/UPOS/XPOS/Feats columns and eventually a parser
  can be used to parse the results.
<p></p>
  The tokenizer is evaluated using F1-score on tokens, multi-word tokens, sentences
  and words. The words are aligned using a word-alignment algorithm
  described in the <a href="http://universaldependencies.org/conll17">CoNLL 2017 Shared Task in UD Parsing</a>.
  The tagger and parser are evaluated on aligned
  words, resulting in F1 scores of Lemmas/UPOS/XPOS/Feats/UAS/LAS.
<p></p>
</li>
<li><code>--tag(ger) [--parse(r)]</code>: The gold segmented and tokenized input
  is tagged (and then optionally parsed using the tagger outputs)
  and then evaluated.
<p></p>
</li>
<li><code>--parse(r)</code>: The gold segmented and tokenized input is parsed using
  gold morphology (Lemmas/UPOS/XPOS/Feats) and evaluated.
<p></p>
</li>
</ul>

<a id="universal_dependencies_20_models" name="universal_dependencies_20_models"></a>
<h2>5.4. Universal Dependencies 2.0 Models</h2>

<p>
Universal Dependencies 2.0 Models are distributed under the
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a> licence.
The models are based solely on
<a href="http://hdl.handle.net/11234/1-1983">Universal Dependencies 2.0</a> treebanks.
The models work in UDPipe version 1.2 and later.
</p>
<p>
Universal Dependencies 2.0 Models are versioned according to the date released
in the format <code>YYMMDD</code>, where <code>YY</code>, <code>MM</code> and <code>DD</code> are two-digit
representation of year, month and day, respectively. The latest version is 170801.
</p>

<a id="universal_dependencies_20_models_download" name="universal_dependencies_20_models_download"></a>
<h3>5.4.1. Download</h3>

<p>
The latest version 170801 of the Universal Dependencies 2.0 models can be downloaded
from <a href="http://hdl.handle.net/11234/1-2364">LINDAT/CLARIN repository</a>.
</p>

<a id="universal_dependencies_20_models_acknowledgements" name="universal_dependencies_20_models_acknowledgements"></a>
<h3>5.4.2. Acknowledgements</h3>

<p>
This work has been partially supported and has been using language resources
and tools developed, stored and distributed by the LINDAT/CLARIN project of the
Ministry of Education, Youth and Sports of the Czech Republic (project <i>LM2015071</i>).
The wark was also partially supported by OP VVV projects
CZ.02.1.01/0.0/0.0/16\_013/0001781 and CZ.02.2.69/0.0/0.0/16\_018/0002373, and
by SVV project number 260 453.
</p>
<p>
The models were trained on <a href="http://hdl.handle.net/11234/1-1983">Universal Dependencies 2.0</a> treebanks.
</p>
<p>
For the UD treebanks which do not contain original plain text version,
raw text is used to train the tokenizer instead. The plain texts
were taken from the <a href="http://hdl.handle.net/11858/00-097C-0000-0022-6133-9">W2C &ndash; Web to Corpus</a>.
</p>

<a id="universal_dependencies_20_models_publications" name="universal_dependencies_20_models_publications"></a>
<h4>5.4.2.1. Publications</h4>

<ul>
<li>(Straka et al. 2017) Milan Straka and Jana Strakov√°. <i><a href="http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf">Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe</a></i>. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Vancouver, Canada, August 2017.
</li>
<li>(Straka et al. 2016) Straka Milan, Hajiƒç Jan, Strakov√° Jana. <i><a href="http://ufal.mff.cuni.cz/~straka/papers/2016-lrec_udpipe.pdf">UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing</a></i>. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Portoro≈æ, Slovenia, May 2016.
</li>
</ul>

<a id="universal_dependencies_20_models_description" name="universal_dependencies_20_models_description"></a>
<h3>5.4.3. Model Description</h3>

<p>
The Universal Dependencies 2.0 models contain 68 models of 50 languages, each consisting of
a tokenizer, tagger, lemmatizer and dependency parser, all trained using
the UD data. Note that we use custom train-dev split, by moving sentences from the beginning
of dev data to the end of train data, until the training data is at least 9 times the dev data.
</p>
<p>
The tokenizer is trained using the <code>SpaceAfter=No</code> features. If the features
are not present in the data, they can be filled in using raw text in the
language in question.
</p>
<p>
The tagger, lemmatizer and parser are trained using gold UD data.
</p>
<p>
Details about model architecture and training process can be found in the
(Straka et al. 2017) paper.
</p>

<a id="universal_dependencies_20_reprodusible_training" name="universal_dependencies_20_reprodusible_training"></a>
<h4>5.4.3.1. Reproducible Training</h4>

<p>
In case you want to train the same models, scripts for downloading and
resplitting UD 2.0 data, precomputed word embedding, raw texts for tokenizers,
all hyperparameter values and training scripts are available in the
second archive on the <a href="http://hdl.handle.net/11234/1-2364">model download page</a>.
</p>

<a id="universal_dependencies_20_models_performance" name="universal_dependencies_20_models_performance"></a>
<h3>5.4.4. Model Performance</h3>

<p>
We present the tagger, lemmatizer and parser performance, measured
on the testing portion of the data, evaluated in three different settings:
using raw text only, using gold tokenization only, and using gold tokenization
plus gold morphology (UPOS, XPOS, FEATS and Lemma).
</p>

<table border="1">
<tr>
<th>Treebank</th>
<th>Mode</th>
<th>Words</th>
<th>Sents</th>
<th>UPOS</th>
<th>XPOS</th>
<th>Feats</th>
<th>AllTags</th>
<th>Lemma</th>
<th>UAS</th>
<th>LAS</th>
</tr>
<tr>
<td>Ancient Greek</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">98.7%</td>
<td align="right">82.4%</td>
<td align="right">72.3%</td>
<td align="right">85.8%</td>
<td align="right">72.3%</td>
<td align="right">82.6%</td>
<td align="right">64.4%</td>
<td align="right">57.8%</td>
</tr>
<tr>
<td>Ancient Greek</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">82.4%</td>
<td align="right">72.4%</td>
<td align="right">85.8%</td>
<td align="right">72.3%</td>
<td align="right">82.7%</td>
<td align="right">64.6%</td>
<td align="right">57.9%</td>
</tr>
<tr>
<td>Ancient Greek</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">69.2%</td>
<td align="right">64.4%</td>
</tr>
<tr>
<td>Ancient Greek-PROIEL</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">47.2%</td>
<td align="right">95.8%</td>
<td align="right">96.0%</td>
<td align="right">88.6%</td>
<td align="right">87.2%</td>
<td align="right">92.6%</td>
<td align="right">71.8%</td>
<td align="right">67.1%</td>
</tr>
<tr>
<td>Ancient Greek-PROIEL</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.8%</td>
<td align="right">96.1%</td>
<td align="right">88.7%</td>
<td align="right">87.2%</td>
<td align="right">92.8%</td>
<td align="right">77.2%</td>
<td align="right">72.3%</td>
</tr>
<tr>
<td>Ancient Greek-PROIEL</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">79.7%</td>
<td align="right">76.1%</td>
</tr>
<tr>
<td>Arabic</td>
<td>Raw text</td>
<td align="right">93.8%</td>
<td align="right">83.1%</td>
<td align="right">88.4%</td>
<td align="right">83.4%</td>
<td align="right">83.5%</td>
<td align="right">82.3%</td>
<td align="right">87.5%</td>
<td align="right">71.7%</td>
<td align="right">65.8%</td>
</tr>
<tr>
<td>Arabic</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.4%</td>
<td align="right">89.5%</td>
<td align="right">89.6%</td>
<td align="right">88.3%</td>
<td align="right">92.6%</td>
<td align="right">81.3%</td>
<td align="right">74.3%</td>
</tr>
<tr>
<td>Arabic</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">82.9%</td>
<td align="right">77.9%</td>
</tr>
<tr>
<td>Basque</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">99.5%</td>
<td align="right">93.2%</td>
<td align="center">-</td>
<td align="right">87.6%</td>
<td align="center">-</td>
<td align="right">93.8%</td>
<td align="right">75.8%</td>
<td align="right">70.7%</td>
</tr>
<tr>
<td>Basque</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">93.3%</td>
<td align="center">-</td>
<td align="right">87.7%</td>
<td align="center">-</td>
<td align="right">93.9%</td>
<td align="right">75.9%</td>
<td align="right">70.8%</td>
</tr>
<tr>
<td>Basque</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">82.3%</td>
<td align="right">78.4%</td>
</tr>
<tr>
<td>Belarusian</td>
<td>Raw text</td>
<td align="right">99.4%</td>
<td align="right">76.8%</td>
<td align="right">88.2%</td>
<td align="right">85.6%</td>
<td align="right">71.7%</td>
<td align="right">68.6%</td>
<td align="right">81.3%</td>
<td align="right">68.0%</td>
<td align="right">60.6%</td>
</tr>
<tr>
<td>Belarusian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.7%</td>
<td align="right">85.7%</td>
<td align="right">72.4%</td>
<td align="right">69.2%</td>
<td align="right">81.5%</td>
<td align="right">69.4%</td>
<td align="right">61.9%</td>
</tr>
<tr>
<td>Belarusian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">76.8%</td>
<td align="right">74.0%</td>
</tr>
<tr>
<td>Bulgarian</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">93.9%</td>
<td align="right">97.6%</td>
<td align="right">94.6%</td>
<td align="right">95.6%</td>
<td align="right">94.0%</td>
<td align="right">94.6%</td>
<td align="right">88.8%</td>
<td align="right">84.8%</td>
</tr>
<tr>
<td>Bulgarian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.7%</td>
<td align="right">94.7%</td>
<td align="right">95.6%</td>
<td align="right">94.1%</td>
<td align="right">94.7%</td>
<td align="right">89.5%</td>
<td align="right">85.5%</td>
</tr>
<tr>
<td>Bulgarian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">92.6%</td>
<td align="right">89.1%</td>
</tr>
<tr>
<td>Catalan</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">99.2%</td>
<td align="right">98.0%</td>
<td align="right">98.0%</td>
<td align="right">97.1%</td>
<td align="right">96.5%</td>
<td align="right">97.9%</td>
<td align="right">88.8%</td>
<td align="right">85.7%</td>
</tr>
<tr>
<td>Catalan</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">98.0%</td>
<td align="right">98.0%</td>
<td align="right">97.2%</td>
<td align="right">96.5%</td>
<td align="right">97.9%</td>
<td align="right">88.8%</td>
<td align="right">85.8%</td>
</tr>
<tr>
<td>Catalan</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.1%</td>
<td align="right">88.7%</td>
</tr>
<tr>
<td>Chinese</td>
<td>Raw text</td>
<td align="right">90.2%</td>
<td align="right">98.8%</td>
<td align="right">84.0%</td>
<td align="right">83.8%</td>
<td align="right">89.0%</td>
<td align="right">82.7%</td>
<td align="right">90.2%</td>
<td align="right">62.9%</td>
<td align="right">58.7%</td>
</tr>
<tr>
<td>Chinese</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">92.2%</td>
<td align="right">92.0%</td>
<td align="right">98.7%</td>
<td align="right">90.8%</td>
<td align="right">100.0%</td>
<td align="right">75.6%</td>
<td align="right">70.1%</td>
</tr>
<tr>
<td>Chinese</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">84.1%</td>
<td align="right">81.4%</td>
</tr>
<tr>
<td>Coptic</td>
<td>Raw text</td>
<td align="right">65.8%</td>
<td align="right">35.7%</td>
<td align="right">62.6%</td>
<td align="right">62.1%</td>
<td align="right">65.7%</td>
<td align="right">62.1%</td>
<td align="right">64.6%</td>
<td align="right">41.1%</td>
<td align="right">39.3%</td>
</tr>
<tr>
<td>Coptic</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.1%</td>
<td align="right">94.3%</td>
<td align="right">99.7%</td>
<td align="right">94.2%</td>
<td align="right">96.2%</td>
<td align="right">83.2%</td>
<td align="right">79.2%</td>
</tr>
<tr>
<td>Coptic</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.1%</td>
<td align="right">84.9%</td>
</tr>
<tr>
<td>Croatian</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">97.0%</td>
<td align="right">95.9%</td>
<td align="center">-</td>
<td align="right">84.3%</td>
<td align="center">-</td>
<td align="right">94.4%</td>
<td align="right">83.6%</td>
<td align="right">77.9%</td>
</tr>
<tr>
<td>Croatian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.0%</td>
<td align="center">-</td>
<td align="right">84.4%</td>
<td align="center">-</td>
<td align="right">94.4%</td>
<td align="right">83.9%</td>
<td align="right">78.1%</td>
</tr>
<tr>
<td>Croatian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.1%</td>
<td align="right">83.2%</td>
</tr>
<tr>
<td>Czech</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">91.6%</td>
<td align="right">98.3%</td>
<td align="right">92.8%</td>
<td align="right">92.1%</td>
<td align="right">91.7%</td>
<td align="right">97.8%</td>
<td align="right">86.8%</td>
<td align="right">83.2%</td>
</tr>
<tr>
<td>Czech</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">98.4%</td>
<td align="right">92.9%</td>
<td align="right">92.2%</td>
<td align="right">91.9%</td>
<td align="right">97.9%</td>
<td align="right">87.7%</td>
<td align="right">84.1%</td>
</tr>
<tr>
<td>Czech</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">90.2%</td>
<td align="right">87.5%</td>
</tr>
<tr>
<td>Czech-CAC</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">99.8%</td>
<td align="right">98.1%</td>
<td align="right">90.6%</td>
<td align="right">89.4%</td>
<td align="right">89.1%</td>
<td align="right">97.0%</td>
<td align="right">86.9%</td>
<td align="right">82.7%</td>
</tr>
<tr>
<td>Czech-CAC</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">98.1%</td>
<td align="right">90.7%</td>
<td align="right">89.5%</td>
<td align="right">89.1%</td>
<td align="right">97.1%</td>
<td align="right">87.0%</td>
<td align="right">82.8%</td>
</tr>
<tr>
<td>Czech-CAC</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">89.7%</td>
<td align="right">86.6%</td>
</tr>
<tr>
<td>Czech-CLTT</td>
<td>Raw text</td>
<td align="right">99.5%</td>
<td align="right">92.3%</td>
<td align="right">96.5%</td>
<td align="right">87.5%</td>
<td align="right">87.8%</td>
<td align="right">87.3%</td>
<td align="right">96.8%</td>
<td align="right">80.2%</td>
<td align="right">76.6%</td>
</tr>
<tr>
<td>Czech-CLTT</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.0%</td>
<td align="right">87.9%</td>
<td align="right">88.3%</td>
<td align="right">87.7%</td>
<td align="right">97.2%</td>
<td align="right">81.0%</td>
<td align="right">77.6%</td>
</tr>
<tr>
<td>Czech-CLTT</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">83.8%</td>
<td align="right">80.8%</td>
</tr>
<tr>
<td>Danish</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">77.9%</td>
<td align="right">95.2%</td>
<td align="center">-</td>
<td align="right">94.2%</td>
<td align="center">-</td>
<td align="right">94.9%</td>
<td align="right">78.4%</td>
<td align="right">74.7%</td>
</tr>
<tr>
<td>Danish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.5%</td>
<td align="center">-</td>
<td align="right">94.5%</td>
<td align="center">-</td>
<td align="right">95.0%</td>
<td align="right">80.4%</td>
<td align="right">76.6%</td>
</tr>
<tr>
<td>Danish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">85.6%</td>
<td align="right">82.7%</td>
</tr>
<tr>
<td>Dutch</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">77.6%</td>
<td align="right">91.4%</td>
<td align="right">88.1%</td>
<td align="right">89.3%</td>
<td align="right">87.0%</td>
<td align="right">89.9%</td>
<td align="right">75.4%</td>
<td align="right">69.6%</td>
</tr>
<tr>
<td>Dutch</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.8%</td>
<td align="right">88.8%</td>
<td align="right">89.9%</td>
<td align="right">87.7%</td>
<td align="right">90.1%</td>
<td align="right">77.0%</td>
<td align="right">71.2%</td>
</tr>
<tr>
<td>Dutch</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">82.9%</td>
<td align="right">79.4%</td>
</tr>
<tr>
<td>Dutch-LassySmall</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">80.4%</td>
<td align="right">97.6%</td>
<td align="center">-</td>
<td align="right">97.2%</td>
<td align="center">-</td>
<td align="right">98.1%</td>
<td align="right">84.4%</td>
<td align="right">82.0%</td>
</tr>
<tr>
<td>Dutch-LassySmall</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.7%</td>
<td align="center">-</td>
<td align="right">97.4%</td>
<td align="center">-</td>
<td align="right">98.2%</td>
<td align="right">87.5%</td>
<td align="right">85.0%</td>
</tr>
<tr>
<td>Dutch-LassySmall</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">89.7%</td>
<td align="right">87.4%</td>
</tr>
<tr>
<td>English</td>
<td>Raw text</td>
<td align="right">99.0%</td>
<td align="right">76.6%</td>
<td align="right">93.5%</td>
<td align="right">92.9%</td>
<td align="right">94.4%</td>
<td align="right">91.5%</td>
<td align="right">96.0%</td>
<td align="right">80.2%</td>
<td align="right">77.2%</td>
</tr>
<tr>
<td>English</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.5%</td>
<td align="right">93.9%</td>
<td align="right">95.4%</td>
<td align="right">92.5%</td>
<td align="right">96.9%</td>
<td align="right">84.3%</td>
<td align="right">81.2%</td>
</tr>
<tr>
<td>English</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.8%</td>
<td align="right">86.0%</td>
</tr>
<tr>
<td>English-LinES</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">86.2%</td>
<td align="right">95.0%</td>
<td align="right">92.7%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">78.6%</td>
<td align="right">74.4%</td>
</tr>
<tr>
<td>English-LinES</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.1%</td>
<td align="right">92.8%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">79.5%</td>
<td align="right">75.3%</td>
</tr>
<tr>
<td>English-LinES</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">84.1%</td>
<td align="right">81.1%</td>
</tr>
<tr>
<td>English-ParTUT</td>
<td>Raw text</td>
<td align="right">99.6%</td>
<td align="right">97.5%</td>
<td align="right">94.2%</td>
<td align="right">94.0%</td>
<td align="right">93.3%</td>
<td align="right">92.0%</td>
<td align="right">96.9%</td>
<td align="right">81.6%</td>
<td align="right">77.9%</td>
</tr>
<tr>
<td>English-ParTUT</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.6%</td>
<td align="right">94.4%</td>
<td align="right">93.6%</td>
<td align="right">92.3%</td>
<td align="right">97.3%</td>
<td align="right">82.1%</td>
<td align="right">78.4%</td>
</tr>
<tr>
<td>English-ParTUT</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">86.4%</td>
<td align="right">84.5%</td>
</tr>
<tr>
<td>Estonian</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">94.2%</td>
<td align="right">91.2%</td>
<td align="right">93.2%</td>
<td align="right">85.0%</td>
<td align="right">83.2%</td>
<td align="right">84.5%</td>
<td align="right">72.4%</td>
<td align="right">65.6%</td>
</tr>
<tr>
<td>Estonian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.3%</td>
<td align="right">93.2%</td>
<td align="right">85.0%</td>
<td align="right">83.3%</td>
<td align="right">84.5%</td>
<td align="right">72.8%</td>
<td align="right">66.0%</td>
</tr>
<tr>
<td>Estonian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">83.1%</td>
<td align="right">79.6%</td>
</tr>
<tr>
<td>Finnish</td>
<td>Raw text</td>
<td align="right">99.7%</td>
<td align="right">86.7%</td>
<td align="right">94.5%</td>
<td align="right">95.7%</td>
<td align="right">91.5%</td>
<td align="right">90.3%</td>
<td align="right">86.5%</td>
<td align="right">80.5%</td>
<td align="right">76.9%</td>
</tr>
<tr>
<td>Finnish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.9%</td>
<td align="right">96.0%</td>
<td align="right">91.8%</td>
<td align="right">90.7%</td>
<td align="right">86.8%</td>
<td align="right">82.0%</td>
<td align="right">78.4%</td>
</tr>
<tr>
<td>Finnish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">86.9%</td>
<td align="right">84.7%</td>
</tr>
<tr>
<td>Finnish-FTB</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">86.4%</td>
<td align="right">92.0%</td>
<td align="right">91.0%</td>
<td align="right">92.5%</td>
<td align="right">89.2%</td>
<td align="right">88.9%</td>
<td align="right">80.1%</td>
<td align="right">75.7%</td>
</tr>
<tr>
<td>Finnish-FTB</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">92.2%</td>
<td align="right">91.3%</td>
<td align="right">92.7%</td>
<td align="right">89.5%</td>
<td align="right">88.9%</td>
<td align="right">81.7%</td>
<td align="right">77.3%</td>
</tr>
<tr>
<td>Finnish-FTB</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.8%</td>
<td align="right">86.5%</td>
</tr>
<tr>
<td>French</td>
<td>Raw text</td>
<td align="right">98.9%</td>
<td align="right">94.6%</td>
<td align="right">95.4%</td>
<td align="center">-</td>
<td align="right">95.5%</td>
<td align="center">-</td>
<td align="right">96.6%</td>
<td align="right">84.2%</td>
<td align="right">80.7%</td>
</tr>
<tr>
<td>French</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.5%</td>
<td align="center">-</td>
<td align="right">96.5%</td>
<td align="center">-</td>
<td align="right">97.6%</td>
<td align="right">85.4%</td>
<td align="right">82.0%</td>
</tr>
<tr>
<td>French</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.4%</td>
<td align="right">86.0%</td>
</tr>
<tr>
<td>French-ParTUT</td>
<td>Raw text</td>
<td align="right">99.0%</td>
<td align="right">97.8%</td>
<td align="right">94.5%</td>
<td align="right">94.2%</td>
<td align="right">91.9%</td>
<td align="right">90.8%</td>
<td align="right">94.3%</td>
<td align="right">82.9%</td>
<td align="right">78.7%</td>
</tr>
<tr>
<td>French-ParTUT</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.6%</td>
<td align="right">95.3%</td>
<td align="right">92.7%</td>
<td align="right">91.6%</td>
<td align="right">95.2%</td>
<td align="right">84.1%</td>
<td align="right">80.2%</td>
</tr>
<tr>
<td>French-ParTUT</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.1%</td>
<td align="right">85.3%</td>
</tr>
<tr>
<td>French-Sequoia</td>
<td>Raw text</td>
<td align="right">99.1%</td>
<td align="right">84.0%</td>
<td align="right">95.9%</td>
<td align="center">-</td>
<td align="right">95.1%</td>
<td align="center">-</td>
<td align="right">96.8%</td>
<td align="right">83.2%</td>
<td align="right">80.6%</td>
</tr>
<tr>
<td>French-Sequoia</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.8%</td>
<td align="center">-</td>
<td align="right">96.0%</td>
<td align="center">-</td>
<td align="right">97.7%</td>
<td align="right">85.1%</td>
<td align="right">82.7%</td>
</tr>
<tr>
<td>French-Sequoia</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.7%</td>
<td align="right">87.4%</td>
</tr>
<tr>
<td>Galician</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">95.8%</td>
<td align="right">97.2%</td>
<td align="right">96.7%</td>
<td align="right">99.7%</td>
<td align="right">96.4%</td>
<td align="right">97.1%</td>
<td align="right">81.0%</td>
<td align="right">77.8%</td>
</tr>
<tr>
<td>Galician</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.2%</td>
<td align="right">96.8%</td>
<td align="right">99.8%</td>
<td align="right">96.4%</td>
<td align="right">97.1%</td>
<td align="right">81.2%</td>
<td align="right">77.9%</td>
</tr>
<tr>
<td>Galician</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">83.1%</td>
<td align="right">80.5%</td>
</tr>
<tr>
<td>Galician-TreeGal</td>
<td>Raw text</td>
<td align="right">98.7%</td>
<td align="right">86.7%</td>
<td align="right">91.1%</td>
<td align="right">87.8%</td>
<td align="right">89.9%</td>
<td align="right">87.0%</td>
<td align="right">92.6%</td>
<td align="right">71.5%</td>
<td align="right">66.3%</td>
</tr>
<tr>
<td>Galician-TreeGal</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">92.4%</td>
<td align="right">88.8%</td>
<td align="right">91.0%</td>
<td align="right">88.0%</td>
<td align="right">93.7%</td>
<td align="right">74.4%</td>
<td align="right">68.7%</td>
</tr>
<tr>
<td>Galician-TreeGal</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">81.5%</td>
<td align="right">77.1%</td>
</tr>
<tr>
<td>German</td>
<td>Raw text</td>
<td align="right">99.7%</td>
<td align="right">79.3%</td>
<td align="right">90.7%</td>
<td align="right">94.7%</td>
<td align="right">80.5%</td>
<td align="right">76.3%</td>
<td align="right">95.4%</td>
<td align="right">74.0%</td>
<td align="right">68.6%</td>
</tr>
<tr>
<td>German</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.2%</td>
<td align="right">95.0%</td>
<td align="right">80.9%</td>
<td align="right">76.7%</td>
<td align="right">95.6%</td>
<td align="right">76.5%</td>
<td align="right">70.7%</td>
</tr>
<tr>
<td>German</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">84.7%</td>
<td align="right">82.2%</td>
</tr>
<tr>
<td>Gothic</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">29.5%</td>
<td align="right">94.2%</td>
<td align="right">94.8%</td>
<td align="right">87.6%</td>
<td align="right">85.6%</td>
<td align="right">92.9%</td>
<td align="right">69.7%</td>
<td align="right">63.5%</td>
</tr>
<tr>
<td>Gothic</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.8%</td>
<td align="right">95.3%</td>
<td align="right">88.0%</td>
<td align="right">86.5%</td>
<td align="right">92.9%</td>
<td align="right">78.8%</td>
<td align="right">72.6%</td>
</tr>
<tr>
<td>Gothic</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">82.2%</td>
<td align="right">78.3%</td>
</tr>
<tr>
<td>Greek</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">88.2%</td>
<td align="right">95.8%</td>
<td align="right">95.8%</td>
<td align="right">90.3%</td>
<td align="right">89.1%</td>
<td align="right">94.5%</td>
<td align="right">84.2%</td>
<td align="right">80.4%</td>
</tr>
<tr>
<td>Greek</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.0%</td>
<td align="right">96.0%</td>
<td align="right">90.5%</td>
<td align="right">89.3%</td>
<td align="right">94.6%</td>
<td align="right">85.0%</td>
<td align="right">81.1%</td>
</tr>
<tr>
<td>Greek</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.9%</td>
<td align="right">85.9%</td>
</tr>
<tr>
<td>Hebrew</td>
<td>Raw text</td>
<td align="right">85.2%</td>
<td align="right">100.0%</td>
<td align="right">80.9%</td>
<td align="right">80.9%</td>
<td align="right">77.6%</td>
<td align="right">76.8%</td>
<td align="right">79.6%</td>
<td align="right">62.2%</td>
<td align="right">57.9%</td>
</tr>
<tr>
<td>Hebrew</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.1%</td>
<td align="right">95.1%</td>
<td align="right">91.3%</td>
<td align="right">90.5%</td>
<td align="right">93.2%</td>
<td align="right">84.5%</td>
<td align="right">78.9%</td>
</tr>
<tr>
<td>Hebrew</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.8%</td>
<td align="right">84.3%</td>
</tr>
<tr>
<td>Hindi</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">99.1%</td>
<td align="right">95.8%</td>
<td align="right">94.9%</td>
<td align="right">90.3%</td>
<td align="right">87.7%</td>
<td align="right">98.0%</td>
<td align="right">91.3%</td>
<td align="right">87.3%</td>
</tr>
<tr>
<td>Hindi</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.8%</td>
<td align="right">94.9%</td>
<td align="right">90.3%</td>
<td align="right">87.7%</td>
<td align="right">98.0%</td>
<td align="right">91.4%</td>
<td align="right">87.3%</td>
</tr>
<tr>
<td>Hindi</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">93.9%</td>
<td align="right">91.0%</td>
</tr>
<tr>
<td>Hungarian</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">96.2%</td>
<td align="right">91.6%</td>
<td align="center">-</td>
<td align="right">70.5%</td>
<td align="center">-</td>
<td align="right">89.3%</td>
<td align="right">74.1%</td>
<td align="right">68.1%</td>
</tr>
<tr>
<td>Hungarian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.8%</td>
<td align="center">-</td>
<td align="right">70.6%</td>
<td align="center">-</td>
<td align="right">89.5%</td>
<td align="right">74.5%</td>
<td align="right">68.5%</td>
</tr>
<tr>
<td>Hungarian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">81.2%</td>
<td align="right">78.5%</td>
</tr>
<tr>
<td>Indonesian</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">92.0%</td>
<td align="right">93.5%</td>
<td align="center">-</td>
<td align="right">99.5%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">80.6%</td>
<td align="right">74.3%</td>
</tr>
<tr>
<td>Indonesian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">93.5%</td>
<td align="center">-</td>
<td align="right">99.6%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">80.8%</td>
<td align="right">74.5%</td>
</tr>
<tr>
<td>Indonesian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">83.1%</td>
<td align="right">79.1%</td>
</tr>
<tr>
<td>Irish</td>
<td>Raw text</td>
<td align="right">99.4%</td>
<td align="right">94.3%</td>
<td align="right">88.0%</td>
<td align="right">86.9%</td>
<td align="right">75.1%</td>
<td align="right">72.7%</td>
<td align="right">85.5%</td>
<td align="right">72.5%</td>
<td align="right">62.4%</td>
</tr>
<tr>
<td>Irish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.5%</td>
<td align="right">87.4%</td>
<td align="right">75.5%</td>
<td align="right">73.1%</td>
<td align="right">86.0%</td>
<td align="right">73.3%</td>
<td align="right">63.1%</td>
</tr>
<tr>
<td>Irish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">78.1%</td>
<td align="right">71.4%</td>
</tr>
<tr>
<td>Italian</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">97.1%</td>
<td align="right">97.2%</td>
<td align="right">97.0%</td>
<td align="right">97.0%</td>
<td align="right">96.1%</td>
<td align="right">97.3%</td>
<td align="right">88.8%</td>
<td align="right">86.1%</td>
</tr>
<tr>
<td>Italian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.4%</td>
<td align="right">97.2%</td>
<td align="right">97.2%</td>
<td align="right">96.3%</td>
<td align="right">97.5%</td>
<td align="right">89.3%</td>
<td align="right">86.6%</td>
</tr>
<tr>
<td>Italian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.3%</td>
<td align="right">89.7%</td>
</tr>
<tr>
<td>Japanese</td>
<td>Raw text</td>
<td align="right">91.9%</td>
<td align="right">95.1%</td>
<td align="right">89.1%</td>
<td align="center">-</td>
<td align="right">91.8%</td>
<td align="center">-</td>
<td align="right">91.1%</td>
<td align="right">78.0%</td>
<td align="right">76.6%</td>
</tr>
<tr>
<td>Japanese</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.6%</td>
<td align="center">-</td>
<td align="right">100.0%</td>
<td align="center">-</td>
<td align="right">99.0%</td>
<td align="right">93.4%</td>
<td align="right">91.5%</td>
</tr>
<tr>
<td>Japanese</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.6%</td>
<td align="right">95.0%</td>
</tr>
<tr>
<td>Kazakh</td>
<td>Raw text</td>
<td align="right">94.0%</td>
<td align="right">84.9%</td>
<td align="right">52.0%</td>
<td align="right">52.1%</td>
<td align="right">47.2%</td>
<td align="right">40.0%</td>
<td align="right">59.2%</td>
<td align="right">40.2%</td>
<td align="right">23.9%</td>
</tr>
<tr>
<td>Kazakh</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">55.4%</td>
<td align="right">55.4%</td>
<td align="right">50.1%</td>
<td align="right">42.2%</td>
<td align="right">63.1%</td>
<td align="right">45.2%</td>
<td align="right">27.0%</td>
</tr>
<tr>
<td>Kazakh</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">60.5%</td>
<td align="right">42.5%</td>
</tr>
<tr>
<td>Korean</td>
<td>Raw text</td>
<td align="right">99.7%</td>
<td align="right">92.7%</td>
<td align="right">94.4%</td>
<td align="right">89.7%</td>
<td align="right">99.3%</td>
<td align="right">89.7%</td>
<td align="right">99.4%</td>
<td align="right">67.4%</td>
<td align="right">60.5%</td>
</tr>
<tr>
<td>Korean</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.7%</td>
<td align="right">90.0%</td>
<td align="right">99.6%</td>
<td align="right">90.0%</td>
<td align="right">99.7%</td>
<td align="right">68.4%</td>
<td align="right">61.5%</td>
</tr>
<tr>
<td>Korean</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">71.7%</td>
<td align="right">65.8%</td>
</tr>
<tr>
<td>Latin</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">98.0%</td>
<td align="right">83.4%</td>
<td align="right">67.6%</td>
<td align="right">72.5%</td>
<td align="right">67.6%</td>
<td align="right">51.2%</td>
<td align="right">56.5%</td>
<td align="right">46.0%</td>
</tr>
<tr>
<td>Latin</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">83.4%</td>
<td align="right">67.6%</td>
<td align="right">72.5%</td>
<td align="right">67.6%</td>
<td align="right">51.2%</td>
<td align="right">56.6%</td>
<td align="right">46.1%</td>
</tr>
<tr>
<td>Latin</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">67.8%</td>
<td align="right">61.5%</td>
</tr>
<tr>
<td>Latin-ITTB</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">82.5%</td>
<td align="right">97.2%</td>
<td align="right">92.7%</td>
<td align="right">93.5%</td>
<td align="right">91.3%</td>
<td align="right">97.8%</td>
<td align="right">79.7%</td>
<td align="right">76.0%</td>
</tr>
<tr>
<td>Latin-ITTB</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.3%</td>
<td align="right">92.8%</td>
<td align="right">93.6%</td>
<td align="right">91.4%</td>
<td align="right">97.9%</td>
<td align="right">81.8%</td>
<td align="right">78.1%</td>
</tr>
<tr>
<td>Latin-ITTB</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.6%</td>
<td align="right">85.2%</td>
</tr>
<tr>
<td>Latin-PROIEL</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">31.0%</td>
<td align="right">94.9%</td>
<td align="right">95.0%</td>
<td align="right">87.7%</td>
<td align="right">86.7%</td>
<td align="right">94.8%</td>
<td align="right">66.1%</td>
<td align="right">60.7%</td>
</tr>
<tr>
<td>Latin-PROIEL</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.2%</td>
<td align="right">95.2%</td>
<td align="right">88.4%</td>
<td align="right">87.4%</td>
<td align="right">95.0%</td>
<td align="right">75.3%</td>
<td align="right">69.4%</td>
</tr>
<tr>
<td>Latin-PROIEL</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">79.0%</td>
<td align="right">75.0%</td>
</tr>
<tr>
<td>Latvian</td>
<td>Raw text</td>
<td align="right">99.2%</td>
<td align="right">97.1%</td>
<td align="right">89.6%</td>
<td align="right">76.2%</td>
<td align="right">83.2%</td>
<td align="right">75.7%</td>
<td align="right">87.6%</td>
<td align="right">69.2%</td>
<td align="right">62.8%</td>
</tr>
<tr>
<td>Latvian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">90.2%</td>
<td align="right">76.8%</td>
<td align="right">84.0%</td>
<td align="right">76.3%</td>
<td align="right">88.3%</td>
<td align="right">70.3%</td>
<td align="right">63.9%</td>
</tr>
<tr>
<td>Latvian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">78.7%</td>
<td align="right">74.9%</td>
</tr>
<tr>
<td>Lithuanian</td>
<td>Raw text</td>
<td align="right">98.2%</td>
<td align="right">92.0%</td>
<td align="right">74.0%</td>
<td align="right">73.0%</td>
<td align="right">68.9%</td>
<td align="right">63.7%</td>
<td align="right">73.5%</td>
<td align="right">44.0%</td>
<td align="right">32.4%</td>
</tr>
<tr>
<td>Lithuanian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">74.6%</td>
<td align="right">73.5%</td>
<td align="right">69.7%</td>
<td align="right">64.2%</td>
<td align="right">74.2%</td>
<td align="right">44.6%</td>
<td align="right">33.0%</td>
</tr>
<tr>
<td>Lithuanian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">55.6%</td>
<td align="right">46.5%</td>
</tr>
<tr>
<td>Norwegian-Bokmaal</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">96.5%</td>
<td align="right">96.9%</td>
<td align="center">-</td>
<td align="right">95.3%</td>
<td align="center">-</td>
<td align="right">96.6%</td>
<td align="right">86.9%</td>
<td align="right">84.1%</td>
</tr>
<tr>
<td>Norwegian-Bokmaal</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.1%</td>
<td align="center">-</td>
<td align="right">95.5%</td>
<td align="center">-</td>
<td align="right">96.8%</td>
<td align="right">87.5%</td>
<td align="right">84.7%</td>
</tr>
<tr>
<td>Norwegian-Bokmaal</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.7%</td>
<td align="right">89.6%</td>
</tr>
<tr>
<td>Norwegian-Nynorsk</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">92.2%</td>
<td align="right">96.5%</td>
<td align="center">-</td>
<td align="right">94.9%</td>
<td align="center">-</td>
<td align="right">96.4%</td>
<td align="right">85.6%</td>
<td align="right">82.5%</td>
</tr>
<tr>
<td>Norwegian-Nynorsk</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.6%</td>
<td align="center">-</td>
<td align="right">95.0%</td>
<td align="center">-</td>
<td align="right">96.5%</td>
<td align="right">86.5%</td>
<td align="right">83.3%</td>
</tr>
<tr>
<td>Norwegian-Nynorsk</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.0%</td>
<td align="right">88.6%</td>
</tr>
<tr>
<td>Old Church Slavonic</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">40.5%</td>
<td align="right">93.8%</td>
<td align="right">93.8%</td>
<td align="right">86.9%</td>
<td align="right">85.7%</td>
<td align="right">91.2%</td>
<td align="right">73.6%</td>
<td align="right">66.9%</td>
</tr>
<tr>
<td>Old Church Slavonic</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.1%</td>
<td align="right">94.1%</td>
<td align="right">87.6%</td>
<td align="right">86.5%</td>
<td align="right">91.2%</td>
<td align="right">81.6%</td>
<td align="right">74.7%</td>
</tr>
<tr>
<td>Old Church Slavonic</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">86.7%</td>
<td align="right">82.2%</td>
</tr>
<tr>
<td>Persian</td>
<td>Raw text</td>
<td align="right">99.7%</td>
<td align="right">98.2%</td>
<td align="right">96.0%</td>
<td align="right">96.0%</td>
<td align="right">96.1%</td>
<td align="right">95.4%</td>
<td align="right">93.5%</td>
<td align="right">83.3%</td>
<td align="right">79.4%</td>
</tr>
<tr>
<td>Persian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.4%</td>
<td align="right">96.3%</td>
<td align="right">96.4%</td>
<td align="right">95.7%</td>
<td align="right">93.8%</td>
<td align="right">83.8%</td>
<td align="right">80.0%</td>
</tr>
<tr>
<td>Persian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.7%</td>
<td align="right">84.9%</td>
</tr>
<tr>
<td>Polish</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">99.7%</td>
<td align="right">95.6%</td>
<td align="right">84.0%</td>
<td align="right">84.1%</td>
<td align="right">83.1%</td>
<td align="right">93.4%</td>
<td align="right">86.7%</td>
<td align="right">80.7%</td>
</tr>
<tr>
<td>Polish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.7%</td>
<td align="right">84.1%</td>
<td align="right">84.2%</td>
<td align="right">83.3%</td>
<td align="right">93.6%</td>
<td align="right">87.0%</td>
<td align="right">81.0%</td>
</tr>
<tr>
<td>Polish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">92.9%</td>
<td align="right">89.5%</td>
</tr>
<tr>
<td>Portuguese</td>
<td>Raw text</td>
<td align="right">99.6%</td>
<td align="right">89.4%</td>
<td align="right">96.4%</td>
<td align="right">72.7%</td>
<td align="right">93.3%</td>
<td align="right">71.6%</td>
<td align="right">96.8%</td>
<td align="right">86.0%</td>
<td align="right">82.6%</td>
</tr>
<tr>
<td>Portuguese</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.8%</td>
<td align="right">73.0%</td>
<td align="right">93.7%</td>
<td align="right">71.9%</td>
<td align="right">97.2%</td>
<td align="right">87.2%</td>
<td align="right">83.6%</td>
</tr>
<tr>
<td>Portuguese</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">89.6%</td>
<td align="right">87.5%</td>
</tr>
<tr>
<td>Portuguese-BR</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">96.8%</td>
<td align="right">97.0%</td>
<td align="right">97.0%</td>
<td align="right">99.7%</td>
<td align="right">97.0%</td>
<td align="right">98.8%</td>
<td align="right">88.5%</td>
<td align="right">86.3%</td>
</tr>
<tr>
<td>Portuguese-BR</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">97.2%</td>
<td align="right">97.2%</td>
<td align="right">99.9%</td>
<td align="right">97.2%</td>
<td align="right">98.9%</td>
<td align="right">88.8%</td>
<td align="right">86.6%</td>
</tr>
<tr>
<td>Portuguese-BR</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">90.5%</td>
<td align="right">89.1%</td>
</tr>
<tr>
<td>Romanian</td>
<td>Raw text</td>
<td align="right">99.7%</td>
<td align="right">93.9%</td>
<td align="right">96.6%</td>
<td align="right">95.9%</td>
<td align="right">96.0%</td>
<td align="right">95.7%</td>
<td align="right">96.5%</td>
<td align="right">85.6%</td>
<td align="right">80.2%</td>
</tr>
<tr>
<td>Romanian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.9%</td>
<td align="right">96.2%</td>
<td align="right">96.3%</td>
<td align="right">96.0%</td>
<td align="right">96.8%</td>
<td align="right">86.2%</td>
<td align="right">80.8%</td>
</tr>
<tr>
<td>Romanian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">87.8%</td>
<td align="right">83.0%</td>
</tr>
<tr>
<td>Russian</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">96.9%</td>
<td align="right">94.7%</td>
<td align="right">94.4%</td>
<td align="right">84.4%</td>
<td align="right">82.8%</td>
<td align="right">75.0%</td>
<td align="right">80.3%</td>
<td align="right">75.5%</td>
</tr>
<tr>
<td>Russian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.8%</td>
<td align="right">94.5%</td>
<td align="right">84.5%</td>
<td align="right">82.9%</td>
<td align="right">75.1%</td>
<td align="right">80.8%</td>
<td align="right">76.0%</td>
</tr>
<tr>
<td>Russian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">84.8%</td>
<td align="right">81.9%</td>
</tr>
<tr>
<td>Russian-SynTagRus</td>
<td>Raw text</td>
<td align="right">99.6%</td>
<td align="right">98.0%</td>
<td align="right">98.0%</td>
<td align="center">-</td>
<td align="right">93.6%</td>
<td align="center">-</td>
<td align="right">95.6%</td>
<td align="right">89.8%</td>
<td align="right">87.2%</td>
</tr>
<tr>
<td>Russian-SynTagRus</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">98.4%</td>
<td align="center">-</td>
<td align="right">93.9%</td>
<td align="center">-</td>
<td align="right">95.9%</td>
<td align="right">90.4%</td>
<td align="right">87.9%</td>
</tr>
<tr>
<td>Russian-SynTagRus</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.8%</td>
<td align="right">90.5%</td>
</tr>
<tr>
<td>Sanskrit</td>
<td>Raw text</td>
<td align="right">88.1%</td>
<td align="right">29.0%</td>
<td align="right">52.0%</td>
<td align="center">-</td>
<td align="right">35.2%</td>
<td align="center">-</td>
<td align="right">50.2%</td>
<td align="right">38.8%</td>
<td align="right">22.5%</td>
</tr>
<tr>
<td>Sanskrit</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">57.6%</td>
<td align="center">-</td>
<td align="right">43.6%</td>
<td align="center">-</td>
<td align="right">60.6%</td>
<td align="right">58.5%</td>
<td align="right">34.3%</td>
</tr>
<tr>
<td>Sanskrit</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">72.9%</td>
<td align="right">58.5%</td>
</tr>
<tr>
<td>Slovak</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">83.5%</td>
<td align="right">93.2%</td>
<td align="right">77.5%</td>
<td align="right">79.7%</td>
<td align="right">77.1%</td>
<td align="right">85.9%</td>
<td align="right">80.4%</td>
<td align="right">75.2%</td>
</tr>
<tr>
<td>Slovak</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">93.3%</td>
<td align="right">77.6%</td>
<td align="right">79.9%</td>
<td align="right">77.2%</td>
<td align="right">86.0%</td>
<td align="right">82.0%</td>
<td align="right">76.9%</td>
</tr>
<tr>
<td>Slovak</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.2%</td>
<td align="right">85.5%</td>
</tr>
<tr>
<td>Slovenian</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">98.9%</td>
<td align="right">96.2%</td>
<td align="right">88.2%</td>
<td align="right">88.5%</td>
<td align="right">87.7%</td>
<td align="right">95.3%</td>
<td align="right">84.9%</td>
<td align="right">81.6%</td>
</tr>
<tr>
<td>Slovenian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">96.2%</td>
<td align="right">88.2%</td>
<td align="right">88.6%</td>
<td align="right">87.7%</td>
<td align="right">95.4%</td>
<td align="right">85.0%</td>
<td align="right">81.7%</td>
</tr>
<tr>
<td>Slovenian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">91.8%</td>
<td align="right">90.5%</td>
</tr>
<tr>
<td>Slovenian-SST</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">17.8%</td>
<td align="right">89.0%</td>
<td align="right">81.1%</td>
<td align="right">81.3%</td>
<td align="right">78.6%</td>
<td align="right">91.6%</td>
<td align="right">53.0%</td>
<td align="right">46.6%</td>
</tr>
<tr>
<td>Slovenian-SST</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">89.4%</td>
<td align="right">81.6%</td>
<td align="right">81.8%</td>
<td align="right">79.3%</td>
<td align="right">91.7%</td>
<td align="right">63.4%</td>
<td align="right">56.0%</td>
</tr>
<tr>
<td>Slovenian-SST</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">75.5%</td>
<td align="right">70.6%</td>
</tr>
<tr>
<td>Spanish</td>
<td>Raw text</td>
<td align="right">99.7%</td>
<td align="right">95.3%</td>
<td align="right">95.5%</td>
<td align="center">-</td>
<td align="right">96.1%</td>
<td align="center">-</td>
<td align="right">95.9%</td>
<td align="right">84.9%</td>
<td align="right">81.4%</td>
</tr>
<tr>
<td>Spanish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.8%</td>
<td align="center">-</td>
<td align="right">96.3%</td>
<td align="center">-</td>
<td align="right">96.1%</td>
<td align="right">85.5%</td>
<td align="right">81.9%</td>
</tr>
<tr>
<td>Spanish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.0%</td>
<td align="right">85.3%</td>
</tr>
<tr>
<td>Spanish-AnCora</td>
<td>Raw text</td>
<td align="right">99.9%</td>
<td align="right">98.0%</td>
<td align="right">98.1%</td>
<td align="right">98.1%</td>
<td align="right">97.5%</td>
<td align="right">96.8%</td>
<td align="right">98.1%</td>
<td align="right">87.7%</td>
<td align="right">84.5%</td>
</tr>
<tr>
<td>Spanish-AnCora</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">98.2%</td>
<td align="right">98.2%</td>
<td align="right">97.5%</td>
<td align="right">96.9%</td>
<td align="right">98.1%</td>
<td align="right">87.8%</td>
<td align="right">84.7%</td>
</tr>
<tr>
<td>Spanish-AnCora</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">90.2%</td>
<td align="right">87.6%</td>
</tr>
<tr>
<td>Swedish</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">94.6%</td>
<td align="right">95.6%</td>
<td align="right">93.9%</td>
<td align="right">94.4%</td>
<td align="right">92.8%</td>
<td align="right">95.5%</td>
<td align="right">81.4%</td>
<td align="right">77.8%</td>
</tr>
<tr>
<td>Swedish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.8%</td>
<td align="right">94.1%</td>
<td align="right">94.6%</td>
<td align="right">93.1%</td>
<td align="right">95.7%</td>
<td align="right">82.1%</td>
<td align="right">78.4%</td>
</tr>
<tr>
<td>Swedish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.0%</td>
<td align="right">85.0%</td>
</tr>
<tr>
<td>Swedish-LinES</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">85.7%</td>
<td align="right">94.8%</td>
<td align="right">92.2%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">80.4%</td>
<td align="right">75.7%</td>
</tr>
<tr>
<td>Swedish-LinES</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.8%</td>
<td align="right">92.3%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">81.3%</td>
<td align="right">76.6%</td>
</tr>
<tr>
<td>Swedish-LinES</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">86.0%</td>
<td align="right">82.6%</td>
</tr>
<tr>
<td>Tamil</td>
<td>Raw text</td>
<td align="right">95.3%</td>
<td align="right">89.2%</td>
<td align="right">82.2%</td>
<td align="right">77.7%</td>
<td align="right">80.9%</td>
<td align="right">77.2%</td>
<td align="right">85.3%</td>
<td align="right">59.5%</td>
<td align="right">52.0%</td>
</tr>
<tr>
<td>Tamil</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">85.8%</td>
<td align="right">81.0%</td>
<td align="right">84.2%</td>
<td align="right">80.3%</td>
<td align="right">89.1%</td>
<td align="right">64.9%</td>
<td align="right">56.5%</td>
</tr>
<tr>
<td>Tamil</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">78.9%</td>
<td align="right">71.8%</td>
</tr>
<tr>
<td>Turkish</td>
<td>Raw text</td>
<td align="right">98.1%</td>
<td align="right">96.8%</td>
<td align="right">92.4%</td>
<td align="right">91.5%</td>
<td align="right">87.3%</td>
<td align="right">85.5%</td>
<td align="right">90.2%</td>
<td align="right">62.9%</td>
<td align="right">55.8%</td>
</tr>
<tr>
<td>Turkish</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">94.0%</td>
<td align="right">93.0%</td>
<td align="right">88.9%</td>
<td align="right">87.0%</td>
<td align="right">91.7%</td>
<td align="right">65.5%</td>
<td align="right">58.0%</td>
</tr>
<tr>
<td>Turkish</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">66.8%</td>
<td align="right">61.1%</td>
</tr>
<tr>
<td>Ukrainian</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">95.1%</td>
<td align="right">88.5%</td>
<td align="right">70.7%</td>
<td align="right">70.9%</td>
<td align="right">67.6%</td>
<td align="right">86.7%</td>
<td align="right">69.9%</td>
<td align="right">61.5%</td>
</tr>
<tr>
<td>Ukrainian</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.6%</td>
<td align="right">70.8%</td>
<td align="right">71.0%</td>
<td align="right">67.7%</td>
<td align="right">86.9%</td>
<td align="right">70.2%</td>
<td align="right">61.8%</td>
</tr>
<tr>
<td>Ukrainian</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">79.0%</td>
<td align="right">74.5%</td>
</tr>
<tr>
<td>Urdu</td>
<td>Raw text</td>
<td align="right">100.0%</td>
<td align="right">98.3%</td>
<td align="right">92.4%</td>
<td align="right">90.5%</td>
<td align="right">80.6%</td>
<td align="right">76.3%</td>
<td align="right">93.0%</td>
<td align="right">84.6%</td>
<td align="right">77.6%</td>
</tr>
<tr>
<td>Urdu</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">92.4%</td>
<td align="right">90.5%</td>
<td align="right">80.7%</td>
<td align="right">76.3%</td>
<td align="right">93.0%</td>
<td align="right">84.7%</td>
<td align="right">77.7%</td>
</tr>
<tr>
<td>Urdu</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">88.2%</td>
<td align="right">83.0%</td>
</tr>
<tr>
<td>Uyghur</td>
<td>Raw text</td>
<td align="right">99.8%</td>
<td align="right">67.2%</td>
<td align="right">74.7%</td>
<td align="right">79.1%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">55.1%</td>
<td align="right">35.0%</td>
</tr>
<tr>
<td>Uyghur</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">75.1%</td>
<td align="right">79.3%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">56.5%</td>
<td align="right">35.8%</td>
</tr>
<tr>
<td>Uyghur</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">62.3%</td>
<td align="right">42.0%</td>
</tr>
<tr>
<td>Vietnamese</td>
<td>Raw text</td>
<td align="right">85.3%</td>
<td align="right">92.9%</td>
<td align="right">77.4%</td>
<td align="right">75.4%</td>
<td align="right">85.1%</td>
<td align="right">75.4%</td>
<td align="right">84.5%</td>
<td align="right">46.9%</td>
<td align="right">42.5%</td>
</tr>
<tr>
<td>Vietnamese</td>
<td>Gold tok</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">89.3%</td>
<td align="right">86.8%</td>
<td align="right">99.6%</td>
<td align="right">86.8%</td>
<td align="right">99.0%</td>
<td align="right">64.4%</td>
<td align="right">57.2%</td>
</tr>
<tr>
<td>Vietnamese</td>
<td>Gold tok+morph</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">70.7%</td>
<td align="right">67.9%</td>
</tr>
</table>

<a id="conll17_shared_task_baseline_ud_20_models" name="conll17_shared_task_baseline_ud_20_models"></a>
<h2>5.5. CoNLL17 Shared Task Baseline UD 2.0 Models</h2>

<p>
As part of CoNLL 2017 Shared Task in UD Parsing, baseline models
for UDPipe were released. The CoNLL 2017 Shared Task models were trained on
most of UD 2.0 treebanks (64 of them) and are distributed under the
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a> licence.
</p>
<p>
Note that the models were released when the test set of UD 2.0 was unknown. Therefore,
the models were trained on a subset of training data only, to allow fair comparison
on the development data (which were unused during training and hyperparameter settings).
Consequently, the performance of the models is not directly comparable to other models.
Details about the concrete data split, hyperparameter values and model
performance are available in the model archive.
</p>

<a id="conll17_shared_task_baseline_ud_20_models_download" name="conll17_shared_task_baseline_ud_20_models_download"></a>
<h3>5.5.1. Download</h3>

<p>
The CoNLL17 Shared Task Baseline UD 2.0 Models can be downloaded
from <a href="http://hdl.handle.net/11234/1-1990">LINDAT/CLARIN repository</a>.
</p>

<a id="conll17_shared_task_baseline_ud_20_models_ackowledgements" name="conll17_shared_task_baseline_ud_20_models_ackowledgements"></a>
<h3>5.5.2. Acknowledgements</h3>

<p>
This work has been partially supported and has been using language resources
and tools developed, stored and distributed by the LINDAT/CLARIN project of the
Ministry of Education, Youth and Sports of the Czech Republic (project <i>LM2015071</i>).
</p>
<p>
The models were trained on a <a href="http://hdl.handle.net/11234/1-1983">Universal Dependencies 2.0</a> treebanks.
</p>

<a id="universal_dependencies_12_models" name="universal_dependencies_12_models"></a>
<h2>5.6. Universal Dependencies 1.2 Models</h2>

<p>
Universal Dependencies 1.2 Models are distributed under the
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a> licence.
The models are based solely on
<a href="http://hdl.handle.net/11234/1-1548">Universal Dependencies 1.2</a> treebanks.
The models work in UDPipe version 1.0.
</p>
<p>
Universal Dependencies 1.2 Models are versioned according to the date released
in the format <code>YYMMDD</code>, where <code>YY</code>, <code>MM</code> and <code>DD</code> are two-digit
representation of year, month and day, respectively. The latest version is 160523.
</p>

<a id="universal_dependencies_12_models_download" name="universal_dependencies_12_models_download"></a>
<h3>5.6.1. Download</h3>

<p>
The latest version 160523 of the Universal Dependencies 1.2 models can be downloaded
from <a href="http://hdl.handle.net/11234/1-1659">LINDAT/CLARIN repository</a>.
</p>

<a id="universal_dependencies_12_models_acknowledgements" name="universal_dependencies_12_models_acknowledgements"></a>
<h3>5.6.2. Acknowledgements</h3>

<p>
This work has been partially supported and has been using language resources
and tools developed, stored and distributed by the LINDAT/CLARIN project of the
Ministry of Education, Youth and Sports of the Czech Republic (project <i>LM2015071</i>).
</p>
<p>
The models were trained on <a href="http://hdl.handle.net/11234/1-1548">Universal Dependencies 1.2</a> treebanks.
</p>
<p>
For the UD treebanks which do not contain original plain text version,
raw text is used to train the tokenizer instead. The plain texts
were taken from the <a href="http://hdl.handle.net/11858/00-097C-0000-0022-6133-9">W2C &ndash; Web to Corpus</a>.
</p>

<a id="universal_dependencies_12_models_publications" name="universal_dependencies_12_models_publications"></a>
<h4>5.6.2.1. Publications</h4>

<ul>
<li>(Straka et al. 2016) Straka Milan, Hajiƒç Jan, Strakov√° Jana. <i>UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing.</i> LREC 2016, Portoro≈æ, Slovenia, May 2016.
</li>
</ul>

<a id="universal_dependencies_12_models_description" name="universal_dependencies_12_models_description"></a>
<h3>5.6.3. Model Description</h3>

<p>
The Universal Dependencies 1.2 models contain 36 models, each consisting of
a tokenizer, tagger, lemmatizer and dependency parser, all trained using
the UD data. The model for Japanese is missing, because we do not have
the license for the required corpus of Mainichi Shinbun 1995.
</p>
<p>
The tokenizer is trained using the <code>SpaceAfter=No</code> features. If the features
are not present in the data, they can be filled in using raw text in the
language in question (surprisingly, quite little data suffices, we use 500kB).
</p>
<p>
The tagger, lemmatizer and parser are trained using gold UD data.
</p>
<p>
Details about model architecture and training process can be found in the
(Straka et al. 2016) paper.
</p>

<a id="universal_dependencies_12_models_performance" name="universal_dependencies_12_models_performance"></a>
<h3>5.6.4. Model Performance</h3>

<p>
We present the tagger, lemmatizer and parser performance, measured
on the testing portion of the data. Only the segmentation and the
tokenization of the testing data is retained before evaluation.
Therefore, the dependency parser is evaluated without gold POS tags.
</p>

<table border="1">
<tr>
<th>Treebank</th>
<th>UPOS</th>
<th>XPOS</th>
<th>Feats</th>
<th>All Tags</th>
<th>Lemma</th>
<th>UAS</th>
<th>LAS</th>
</tr>
<tr>
<td>Ancient Greek</td>
<td align="right">91.1%</td>
<td align="right">77.8%</td>
<td align="right">88.7%</td>
<td align="right">77.7%</td>
<td align="right">86.9%</td>
<td align="right">68.1%</td>
<td align="right">61.6%</td>
</tr>
<tr>
<td>Ancient Greek-PROIEL</td>
<td align="right">96.7%</td>
<td align="right">96.4%</td>
<td align="right">89.3%</td>
<td align="right">88.4%</td>
<td align="right">93.4%</td>
<td align="right">75.8%</td>
<td align="right">69.6%</td>
</tr>
<tr>
<td>Arabic</td>
<td align="right">98.8%</td>
<td align="right">97.7%</td>
<td align="right">97.8%</td>
<td align="right">97.6%</td>
<td align="center">-</td>
<td align="right">80.4%</td>
<td align="right">75.6%</td>
</tr>
<tr>
<td>Basque</td>
<td align="right">93.3%</td>
<td align="center">-</td>
<td align="right">87.2%</td>
<td align="right">85.4%</td>
<td align="right">93.5%</td>
<td align="right">74.8%</td>
<td align="right">69.5%</td>
</tr>
<tr>
<td>Bulgarian</td>
<td align="right">97.8%</td>
<td align="right">94.8%</td>
<td align="right">94.4%</td>
<td align="right">93.1%</td>
<td align="right">94.6%</td>
<td align="right">89.0%</td>
<td align="right">84.2%</td>
</tr>
<tr>
<td>Croatian</td>
<td align="right">94.9%</td>
<td align="center">-</td>
<td align="right">85.5%</td>
<td align="right">85.0%</td>
<td align="right">93.1%</td>
<td align="right">78.6%</td>
<td align="right">71.0%</td>
</tr>
<tr>
<td>Czech</td>
<td align="right">98.4%</td>
<td align="right">93.2%</td>
<td align="right">92.6%</td>
<td align="right">92.2%</td>
<td align="right">97.8%</td>
<td align="right">86.9%</td>
<td align="right">83.0%</td>
</tr>
<tr>
<td>Danish</td>
<td align="right">95.8%</td>
<td align="center">-</td>
<td align="right">94.8%</td>
<td align="right">93.6%</td>
<td align="right">95.2%</td>
<td align="right">78.6%</td>
<td align="right">74.8%</td>
</tr>
<tr>
<td>Dutch</td>
<td align="right">89.7%</td>
<td align="right">88.7%</td>
<td align="right">91.2%</td>
<td align="right">86.4%</td>
<td align="right">88.9%</td>
<td align="right">78.1%</td>
<td align="right">70.7%</td>
</tr>
<tr>
<td>English</td>
<td align="right">94.5%</td>
<td align="right">93.8%</td>
<td align="right">95.4%</td>
<td align="right">92.5%</td>
<td align="right">97.0%</td>
<td align="right">84.2%</td>
<td align="right">80.6%</td>
</tr>
<tr>
<td>Estonian</td>
<td align="right">88.0%</td>
<td align="right">73.7%</td>
<td align="right">80.0%</td>
<td align="right">73.6%</td>
<td align="right">77.0%</td>
<td align="right">79.9%</td>
<td align="right">71.5%</td>
</tr>
<tr>
<td>Finnish</td>
<td align="right">94.9%</td>
<td align="right">96.0%</td>
<td align="right">93.2%</td>
<td align="right">92.1%</td>
<td align="right">86.8%</td>
<td align="right">81.0%</td>
<td align="right">76.5%</td>
</tr>
<tr>
<td>Finnish-FTB</td>
<td align="right">94.0%</td>
<td align="right">91.6%</td>
<td align="right">93.3%</td>
<td align="right">91.2%</td>
<td align="right">89.1%</td>
<td align="right">81.5%</td>
<td align="right">76.9%</td>
</tr>
<tr>
<td>French</td>
<td align="right">95.8%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">95.8%</td>
<td align="center">-</td>
<td align="right">82.8%</td>
<td align="right">78.4%</td>
</tr>
<tr>
<td>German</td>
<td align="right">90.5%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">90.5%</td>
<td align="center">-</td>
<td align="right">78.2%</td>
<td align="right">72.2%</td>
</tr>
<tr>
<td>Gothic</td>
<td align="right">95.5%</td>
<td align="right">95.7%</td>
<td align="right">88.0%</td>
<td align="right">86.3%</td>
<td align="right">93.4%</td>
<td align="right">76.4%</td>
<td align="right">68.2%</td>
</tr>
<tr>
<td>Greek</td>
<td align="right">97.3%</td>
<td align="right">97.3%</td>
<td align="right">92.8%</td>
<td align="right">91.7%</td>
<td align="right">94.8%</td>
<td align="right">80.3%</td>
<td align="right">76.5%</td>
</tr>
<tr>
<td>Hebrew</td>
<td align="right">94.9%</td>
<td align="right">94.9%</td>
<td align="right">91.3%</td>
<td align="right">90.5%</td>
<td align="center">-</td>
<td align="right">82.6%</td>
<td align="right">76.8%</td>
</tr>
<tr>
<td>Hindi</td>
<td align="right">95.8%</td>
<td align="right">94.8%</td>
<td align="right">90.2%</td>
<td align="right">87.7%</td>
<td align="right">98.0%</td>
<td align="right">91.7%</td>
<td align="right">87.5%</td>
</tr>
<tr>
<td>Hungarian</td>
<td align="right">92.6%</td>
<td align="center">-</td>
<td align="right">89.9%</td>
<td align="right">88.9%</td>
<td align="right">86.9%</td>
<td align="right">77.0%</td>
<td align="right">70.6%</td>
</tr>
<tr>
<td>Indonesian</td>
<td align="right">93.5%</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="right">93.5%</td>
<td align="center">-</td>
<td align="right">79.9%</td>
<td align="right">73.3%</td>
</tr>
<tr>
<td>Irish</td>
<td align="right">91.8%</td>
<td align="right">90.3%</td>
<td align="right">79.4%</td>
<td align="right">76.6%</td>
<td align="right">87.3%</td>
<td align="right">74.4%</td>
<td align="right">66.1%</td>
</tr>
<tr>
<td>Italian</td>
<td align="right">97.2%</td>
<td align="right">97.0%</td>
<td align="right">97.1%</td>
<td align="right">96.2%</td>
<td align="right">97.7%</td>
<td align="right">88.6%</td>
<td align="right">85.8%</td>
</tr>
<tr>
<td>Latin</td>
<td align="right">91.2%</td>
<td align="right">75.8%</td>
<td align="right">79.3%</td>
<td align="right">75.6%</td>
<td align="right">79.9%</td>
<td align="right">57.1%</td>
<td align="right">46.7%</td>
</tr>
<tr>
<td>Latin-ITT</td>
<td align="right">98.8%</td>
<td align="right">94.0%</td>
<td align="right">94.6%</td>
<td align="right">93.8%</td>
<td align="right">98.3%</td>
<td align="right">79.9%</td>
<td align="right">76.4%</td>
</tr>
<tr>
<td>Latin-PROIEL</td>
<td align="right">96.4%</td>
<td align="right">96.0%</td>
<td align="right">88.9%</td>
<td align="right">88.2%</td>
<td align="right">95.3%</td>
<td align="right">75.3%</td>
<td align="right">68.3%</td>
</tr>
<tr>
<td>Norwegian</td>
<td align="right">97.2%</td>
<td align="center">-</td>
<td align="right">95.5%</td>
<td align="right">94.7%</td>
<td align="right">96.9%</td>
<td align="right">86.7%</td>
<td align="right">84.1%</td>
</tr>
<tr>
<td>Old Church Slavonic</td>
<td align="right">95.3%</td>
<td align="right">95.1%</td>
<td align="right">89.1%</td>
<td align="right">88.2%</td>
<td align="right">92.9%</td>
<td align="right">80.6%</td>
<td align="right">73.4%</td>
</tr>
<tr>
<td>Persian</td>
<td align="right">97.0%</td>
<td align="right">96.3%</td>
<td align="right">96.5%</td>
<td align="right">96.2%</td>
<td align="center">-</td>
<td align="right">83.8%</td>
<td align="right">79.4%</td>
</tr>
<tr>
<td>Polish</td>
<td align="right">95.8%</td>
<td align="right">84.0%</td>
<td align="right">84.1%</td>
<td align="right">83.8%</td>
<td align="right">92.8%</td>
<td align="right">86.3%</td>
<td align="right">79.6%</td>
</tr>
<tr>
<td>Portuguese</td>
<td align="right">97.6%</td>
<td align="right">92.3%</td>
<td align="right">95.3%</td>
<td align="right">92.0%</td>
<td align="right">97.8%</td>
<td align="right">85.8%</td>
<td align="right">81.9%</td>
</tr>
<tr>
<td>Romanian</td>
<td align="right">89.0%</td>
<td align="right">81.0%</td>
<td align="right">82.3%</td>
<td align="right">81.0%</td>
<td align="right">75.3%</td>
<td align="right">68.6%</td>
<td align="right">56.9%</td>
</tr>
<tr>
<td>Slovenian</td>
<td align="right">95.7%</td>
<td align="right">88.2%</td>
<td align="right">88.6%</td>
<td align="right">87.5%</td>
<td align="right">95.0%</td>
<td align="right">84.1%</td>
<td align="right">80.3%</td>
</tr>
<tr>
<td>Spanish</td>
<td align="right">95.3%</td>
<td align="center">-</td>
<td align="right">95.9%</td>
<td align="right">93.4%</td>
<td align="right">96.3%</td>
<td align="right">84.2%</td>
<td align="right">80.3%</td>
</tr>
<tr>
<td>Swedish</td>
<td align="right">95.8%</td>
<td align="right">93.9%</td>
<td align="right">94.8%</td>
<td align="right">93.2%</td>
<td align="right">95.5%</td>
<td align="right">81.4%</td>
<td align="right">77.1%</td>
</tr>
<tr>
<td>Tamil</td>
<td align="right">85.9%</td>
<td align="right">80.8%</td>
<td align="right">84.3%</td>
<td align="right">80.2%</td>
<td align="right">88.0%</td>
<td align="right">67.2%</td>
<td align="right">58.8%</td>
</tr>
</table>

<a id="api_reference" name="api_reference"></a>
<h1>6. UDPipe API Reference</h1>

<p>
The UDPipe API is defined in header <code>udpipe.h</code> and resides in
<code>ufal::udpipe</code> namespace. The API allows only using existing models,
for custom model creation you have to use the <code>train_parser</code> binary.
</p>
<p>
The strings used in the UDPipe API are always UTF-8 encoded (except from
file paths, whose encoding is system dependent).
</p>

<a id="versioning" name="versioning"></a>
<h2>6.1. UDPipe Versioning</h2>

<p>
UDPipe is versioned using <a href="http://semver.org/">Semantic Versioning</a>.
Therefore, a version consists of three numbers <i>major.minor.patch</i>, optionally
followed by a hyphen and pre-release version info, with the following semantics:
</p>

<ul>
<li>Stable versions have no pre-release version info, development have non-empty
  pre-release version info.
</li>
<li>Two versions with the same <i>major.minor</i> have the same API with the same
  behaviour, apart from bugs. Therefore, if only <i>patch</i> is increased, the
  new version is only a bug-fix release.
</li>
<li>If two versions <i>v</i> and <i>u</i> have the same <i>major</i>, but <i>minor(v)</i> is
  greater than <i>minor(u)</i>, version <i>v</i> contains only additions to the API.
  In other words, the API of <i>u</i> is all present in <i>v</i> with the same
  behaviour (once again apart from bugs). It is therefore safe to upgrade to
  a newer UDPipe version with the same <i>major</i>.
</li>
<li>If two versions differ in <i>major</i>, their API may differ in any way.
</li>
</ul>

<p>
Models created by UDPipe have the same behaviour in all UDPipe
versions with same <i>major</i>, apart from obvious bugfixes. On the other hand,
models created from the same data by different <i>major.minor</i> UDPipe
versions may have different behaviour.
</p>

<a id="string_piece" name="string_piece"></a>
<h2>6.2. Struct string_piece</h2>

<pre>
struct string_piece {
  const char* str;
  size_t len;

  string_piece();
  string_piece(const char* str);
  string_piece(const char* str, size_t len);
  string_piece(const std::string&amp; str);
}
</pre>

<p>
The <a href="#string_piece"><code>string_piece</code></a> is used for efficient string passing. The string
referenced in <a href="#string_piece"><code>string_piece</code></a> is not owned by it, so users have to make sure
the referenced string exists as long as the <a href="#string_piece"><code>string_piece</code></a>.
</p>

<a id="token" name="token"></a>
<h2>6.3. Class token</h2>

<pre>
class token {
 public:
  string form;
  string misc;

  token(<A HREF="#string_piece">string_piece</A> form = string_piece(), <A HREF="#string_piece">string_piece</A> misc = string_piece());

  // CoNLL-U defined SpaceAfter=No feature
  bool <A HREF="#token_get_space_after">get_space_after</A>() const;
  void <A HREF="#token_set_space_after">set_space_after</A>(bool space_after);

  // UDPipe-specific all-spaces-preserving SpacesBefore and SpacesAfter features
  void <A HREF="#token_get_spaces_before">get_spaces_before</A>(string&amp; spaces_before) const;
  void <A HREF="#token_set_spaces_before">set_spaces_before</A>(<A HREF="#string_piece">string_piece</A> spaces_before);
  void <A HREF="#token_get_spaces_after">get_spaces_after</A>(string&amp; spaces_after) const;
  void <A HREF="#token_set_spaces_after">set_spaces_after</A>(<A HREF="#string_piece">string_piece</A> spaces_after);
  void <A HREF="#token_get_spaces_in_token">get_spaces_in_token</A>(string&amp; spaces_in_token) const;
  void <A HREF="#token_set_spaces_in_token">set_spaces_in_token</A>(<A HREF="#string_piece">string_piece</A> spaces_in_token);

  // UDPipe-specific TokenRange feature
  bool <A HREF="#token_get_token_range">get_token_range</A>(size_t&amp; start, size_t&amp; end) const;
  void <A HREF="#token_set_token_range">set_token_range</A>(size_t start, size_t end);
};
</pre>

<p>
The <a href="#token"><code>token</code></a> class represents a sentence token,
with <code>form</code> and <code>misc</code> fields corresponding to <a href="http://universaldependencies.org/docs/format.html">CoNLL-U fields</a>.
The <a href="#token"><code>token</code></a> class acts mostly as a parent to <a href="#word"><code>word</code></a>
and <a href="#multiword_token"><code>multiword_token</code></a> classes.
</p>
<p>
The class also offers several methods for manipulating features in the <code>misc</code> field.
Notably, UDPipe uses custom <code>misc</code> fields to store all spaces in the original
document. This markup is backward compatible with CoNLL-U v2 <code>SpaceAfter=No</code> feature.
This markup can be utilized by <code>plaintext</code> output format, which allows reconstructing
the original document.
</p>
<p style="margin-bottom:0">
The markup uses the following <code>misc</code> fields:
</p>
<ul style="margin-top:0">
<li><code>SpacesBefore=content</code> (by default empty): spaces/other content preceding the token
</li>
<li><code>SpacesAfter=content</code> (by default a space if <code>SpaceAfter=No</code> feature is not present, empty otherwise):
  spaces/other content following the token
</li>
<li><code>SpacesInToken=content</code> (by default equal to the FORM of the token): FORM of the token
  including original spaces (this is needed only if tokens are allowed to contain spaces and
  a token contains a tab or newline characters)
</li>
</ul>

<p style="margin-bottom:0">
The <code>content</code> of all above three fields must be escaped to allow storing tabs and newlines.
The following C-like schema is used:
</p>
<ul style="margin-top:0">
<li><code>\s</code>: space
</li>
<li><code>\t</code>: tab
</li>
<li><code>\r</code>: CR character
</li>
<li><code>\n</code>: LF character
</li>
<li><code>\p</code>: | (pipe character)
</li>
<li><code>\\</code>: \ (backslash character)
</li>
</ul>

<a id="token_get_space_after" name="token_get_space_after"></a>
<h3>6.3.1. token::get_space_after()</h3>

<pre>
bool get_space_after() const;
</pre>

<p>
Returns <code>true</code> if the token should be followed by a spaces, <code>false</code> if not,
according to the absence or presence of the <code>SpaceAfter=No</code> feature in the <code>misc</code> field.
</p>

<a id="token_set_space_after" name="token_set_space_after"></a>
<h3>6.3.2. token::set_space_after()</h3>

<pre>
void set_space_after(bool space_after);
</pre>

<p>
Adds or removes the <code>SpaceAfter=No</code> feature in the <code>misc</code> field.
</p>

<a id="token_get_spaces_before" name="token_get_spaces_before"></a>
<h3>6.3.3. token::get_spaces_before()</h3>

<pre>
void get_spaces_before(string&amp; spaces_before) const;
</pre>

<p>
Return spaces preceding current token, stored in the <code>SpacesBefore</code>
feature in the <code>misc</code> field. If <code>SpacesBefore</code> is not present, empty string
is returned.
</p>

<a id="token_set_spaces_before" name="token_set_spaces_before"></a>
<h3>6.3.4. token::set_spaces_before()</h3>

<pre>
void set_spaces_before(<A HREF="#string_piece">string_piece</A> spaces_before);
</pre>

<p>
Set the <code>SpacesBefore</code> feature in the <code>misc</code> field.
</p>

<a id="token_get_spaces_after" name="token_get_spaces_after"></a>
<h3>6.3.5. token::get_spaces_after()</h3>

<pre>
void get_spaces_after(string&amp; spaces_after) const;
</pre>

<p>
Return spaces after current token, stored in the <code>SpacesAfter</code>
feature in the <code>misc</code> field.
</p>
<p>
If <code>SpacesAfter</code> is not present and <code>SpaceAfter=No</code> is present,
return an empty string; if neither feature is present, one space is returned.
</p>

<a id="token_set_spaces_after" name="token_set_spaces_after"></a>
<h3>6.3.6. token::set_spaces_after()</h3>

<pre>
void set_spaces_after(<A HREF="#string_piece">string_piece</A> spaces_after);
</pre>

<p>
Set the <code>SpacesAfter</code> and <code>SpaceAfter=No</code> features in the <code>misc</code> field.
</p>

<a id="token_get_spaces_in_token" name="token_get_spaces_in_token"></a>
<h3>6.3.7. token::get_spaces_in_token()</h3>

<pre>
void get_spaces_in_token(string&amp; spaces_in_token) const;
</pre>

<p>
Return the value of the <code>SpacesInToken</code> feature, if present.
Otherwise, empty string is returned.
</p>

<a id="token_set_spaces_in_token" name="token_set_spaces_in_token"></a>
<h3>6.3.8. token::set_spaces_in_token()</h3>

<pre>
void set_spaces_in_token(<A HREF="#string_piece">string_piece</A> spaces_in_token);
</pre>

<p>
Set the <code>SpacesInToken</code> feature in the <code>misc</code> field.
</p>

<a id="token_get_token_range" name="token_get_token_range"></a>
<h3>6.3.9. token::get_token_range()</h3>

<pre>
bool get_token_range(size_t&amp; start, size_t&amp; end) const;
</pre>

<p>
If present, return the value of the <code>TokenRange</code> feature in the <code>misc</code> field.
The format of the feature (inspired by Python) is <code>TokenRange=start:end</code>,
where <code>start</code> is zero-based document-level index of the start of the token
(counted in Unicode characters) and <code>end</code> is zero-based document-level index
of the first character following the token (i.e., the length of the token is <code>end-start</code>).
</p>

<a id="token_set_token_range" name="token_set_token_range"></a>
<h3>6.3.10. token::set_token_range()</h3>

<pre>
void set_token_range(size_t start, size_t end);
</pre>

<p>
Set the <code>TokenRange</code> feature in the <code>misc</code> field. If <code>string::npos</code>
is passed in the <code>start</code> argument, <code>TokenRange</code> feature is removed
from the <code>misc</code> field.
</p>

<a id="word" name="word"></a>
<h2>6.4. Class word</h2>

<pre>
class word : public token {
 public:
  // form and misc are inherited from token
  int id;         // 0 is root, &gt;0 is sentence word, &lt;0 is undefined
  string lemma;   // lemma
  string upostag; // universal part-of-speech tag
  string xpostag; // language-specific part-of-speech tag
  string feats;   // list of morphological features
  int head;       // head, 0 is root, &lt;0 is undefined
  string deprel;  // dependency relation to the head
  string deps;    // secondary dependencies

  vector&lt;int&gt; children;

  word(int id = -1, <A HREF="#string_piece">string_piece</A> form = string_piece());
};
</pre>

<p>
The <a href="#word"><code>word</code></a> class represents a sentence word.
The <a href="#word"><code>word</code></a> fields correspond to <a href="http://universaldependencies.org/docs/format.html">CoNLL-U fields</a>,
with the <code>children</code> field representing the opposite direction of
<code>head</code> links (the elements of the <code>children</code> array are in ascending order).
</p>

<a id="multiword_token" name="multiword_token"></a>
<h2>6.5. Class multiword_token</h2>

<pre>
class multiword_token : public token {
 public:
  // form and misc are inherited from token
  int id_first, id_last;

  multiword_token(int id_first = -1, int id_last = -1, <A HREF="#string_piece">string_piece</A> form = string_piece(), <A HREF="#string_piece">string_piece</A> misc = string_piece());
};
</pre>

<p>
The <a href="#multiword_token"><code>multiword_token</code></a> represents a multi-word token
described in <a href="http://universaldependencies.org/docs/format.html">CoNLL-U format</a>.
The multi-word token has a <code>form</code> and a <code>misc</code> field, other CoNLL-U word
fields are guaranteed to be empty.
</p>

<a id="empty_node" name="empty_node"></a>
<h2>6.6. Class empty_node</h2>

<pre>
class empty_node {
 public:
  int id;         // 0 is root, &gt;0 is sentence word, &lt;0 is undefined
  int index;      // index for the current id, should be numbered from 1, 0=undefined
  string form;    // form
  string lemma;   // lemma
  string upostag; // universal part-of-speech tag
  string xpostag; // language-specific part-of-speech tag
  string feats;   // list of morphological features
  string deps;    // secondary dependencies
  string misc;    // miscellaneous information

  empty_node(int id = -1, int index = 0) : id(id), index(index) {}
};
</pre>

<p>
The <a href="#empty_node"><code>empty_node</code></a> class represents an empty node from CoNLL-U 2.0,
with the fields corresponding to <a href="http://universaldependencies.org/docs/format.html">CoNLL-U fields</a>.
For a specified <code>id</code>, the <code>index</code> are numbered sequentially from 1.
</p>

<a id="sentence" name="sentence"></a>
<h2>6.7. Class sentence</h2>

<pre>
class sentence {
 public:
  sentence();

  vector&lt;<A HREF="#word">word</A>&gt; words;
  vector&lt;<A HREF="#multiword_token">multiword_token</A>&gt; multiword_tokens;
  vector&lt;empty_node&gt; empty_nodes;
  vector&lt;string&gt; comments;
  static const string root_form;

  // Basic sentence modifications
  bool <A HREF="#sentence_empty">empty</A>();
  void <A HREF="#sentence_clear">clear</A>();
  <A HREF="#word">word</A>&amp; <A HREF="#sentence_add_word">add_word</A>(<A HREF="#string_piece">string_piece</A> form = string_piece());
  void <A HREF="#sentence_set_head">set_head</A>(int id, int head, const string&amp; deprel);
  void <A HREF="#sentence_unlink_all_words">unlink_all_words</A>();

  // CoNLL-U defined comments
  bool <A HREF="#sentence_get_new_doc">get_new_doc</A>(string* id = nullptr) const;
  void <A HREF="#sentence_set_new_doc">set_new_doc</A>(bool new_doc, <A HREF="#string_piece">string_piece</A> id = string_piece());
  bool <A HREF="#sentence_get_new_par">get_new_par</A>(string* id = nullptr) const;
  void <A HREF="#sentence_set_new_par">set_new_par</A>(bool new_par, <A HREF="#string_piece">string_piece</A> id = string_piece());
  bool <A HREF="#sentence_get_sent_id">get_sent_id</A>(string&amp; id) const;
  void <A HREF="#sentence_set_sent_id">set_sent_id</A>(<A HREF="#string_piece">string_piece</A> id);
  bool <A HREF="#sentence_get_text">get_text</A>(string&amp; text) const;
  void <A HREF="#sentence_set_text">set_text</A>(<A HREF="#string_piece">string_piece</A> text);
};
</pre>

<p style="margin-bottom:0">
The <a href="#sentence"><code>sentence</code></a> class represents a sentence <a href="http://universaldependencies.org/docs/format.html">CoNLL-U sentence</a>,
which consists of:
</p>
<ul style="margin-top:0">
<li>sequence of <a href="#word"><code>word</code></a>s stored in ascending order, with the first word
  (with index 0) always being a technical root with form <code>root_form</code>
</li>
<li>sequence of <a href="#multiword_token"><code>multiword_token</code></a>s also stored in ascending
  order
</li>
<li>sequence of <a href="#empty_node"><code>empty_node</code></a>s also stored in ascending order
</li>
<li>comments
</li>
</ul>

<p>
Although you can manipulate the <code>words</code> directly, the
<a href="#sentence"><code>sentence</code></a> class offers several simple node manipulation methods.
There are also several methods manipulating CoNLL-U v2 comments.
</p>

<a id="sentence_empty" name="sentence_empty"></a>
<h3>6.7.1. sentence::empty()</h3>

<pre>
bool empty();
</pre>

<p>
Returns <code>true</code> if the sentence is empty. i.e., if it contains only a technical root node.
</p>

<a id="sentence_clear" name="sentence_clear"></a>
<h3>6.7.2. sentence::clear()</h3>

<pre>
void clear();
</pre>

<p>
Removes all words, multi-word tokens and comments (only the technical root <code>word</code> is kept).
</p>

<a id="sentence_add_word" name="sentence_add_word"></a>
<h3>6.7.3. sentence::add_word()</h3>

<pre>
<A HREF="#word">word</A>&amp; add_word(<A HREF="#string_piece">string_piece</A> form = string_piece());
</pre>

<p>
Adds a new word to the sentence. The new word has first unused <code>id</code>,
specified <code>form</code> and is not linked to any other node. Reference to the new
word is returned so that other fields can be also filled.
</p>

<a id="sentence_set_head" name="sentence_set_head"></a>
<h3>6.7.4. sentence:set_head()</h3>

<pre>
void set_head(int id, int head, const std::string&amp; deprel);
</pre>

<p>
Link the word <code>id</code> to the word <code>head</code>, with the specified dependency relation.
If the <code>head</code> is negative, the word <code>id</code> is unlinked from its current head,
if any.
</p>

<a id="sentence_unlink_all_words" name="sentence_unlink_all_words"></a>
<h3>6.7.5. sentence::unlink_all_words()</h3>

<pre>
void unlink_all_words();
</pre>

<p>
Unlink all words.
</p>

<a id="sentence_get_new_doc" name="sentence_get_new_doc"></a>
<h3>6.7.6. sentence::get_new_doc()</h3>

<pre>
bool get_new_doc(string* id = nullptr) const;
</pre>

<p>
Return <code>true</code> if <code># newdoc</code> comment is present. Optionally,
document id is also returned (in <code># newdoc id = ...</code> format).
</p>

<a id="sentence_set_new_doc" name="sentence_set_new_doc"></a>
<h3>6.7.7. sentence::set_new_doc()</h3>

<pre>
void set_new_doc(bool new_doc, <A HREF="#string_piece">string_piece</A> id = string_piece());
</pre>

<p>
Adds/removes <code># newdoc</code> comment, optionally with a given
document id.
</p>

<a id="sentence_get_new_par" name="sentence_get_new_par"></a>
<h3>6.7.8. sentence::get_new_par()</h3>

<pre>
bool get_new_par(string* id = nullptr) const;
</pre>

<p>
Return <code>true</code> if <code># newpar</code> comment is present. Optionally,
paragraph id is also returned (in <code># newpar id = ...</code> format).
</p>

<a id="sentence_set_new_par" name="sentence_set_new_par"></a>
<h3>6.7.9. sentence::set_new_par()</h3>

<pre>
void set_new_par(bool new_par, <A HREF="#string_piece">string_piece</A> id = string_piece());
</pre>

<p>
Adds/removes <code># newpar</code> comment, optionally with a given
paragraph id.
</p>

<a id="sentence_get_sent_id" name="sentence_get_sent_id"></a>
<h3>6.7.10. sentence::get_sent_id()</h3>

<pre>
bool get_sent_id(string&amp; id) const;
</pre>

<p>
Return <code>true</code> if <code># sent_id = ...</code> comment is present,
and fill given <code>id</code> with sentence id. Otherwise, return <code>false</code>
and clear <code>id</code>.
</p>

<a id="sentence_set_sent_id" name="sentence_set_sent_id"></a>
<h3>6.7.11. sentence::set_sent_id()</h3>

<pre>
void set_sent_id(<A HREF="#string_piece">string_piece</A> id);
</pre>

<p>
Set the <code># sent_id = ...</code> comment using given sentence id;
if the sentence id is empty, remove all present <code># sent_id</code> comment.
</p>

<a id="sentence_get_text" name="sentence_get_text"></a>
<h3>6.7.12. sentence::get_text()</h3>

<pre>
bool get_text(string&amp; text) const;
</pre>

<p>
Return <code>true</code> if <code># text = ...</code> comment is present,
and fill given <code>text</code> with sentence text. Otherwise, return <code>false</code>
and clear <code>text</code>.
</p>

<a id="sentence_set_text" name="sentence_set_text"></a>
<h3>6.7.13. sentence::set_text()</h3>

<pre>
void set_text(<A HREF="#string_piece">string_piece</A> text);
</pre>

<p>
Set the <code># text = ...</code> comment using given text;
if the given text is empty, remove all present <code># text</code> comment.
</p>

<a id="input_format" name="input_format"></a>
<h2>6.8. Class input_format</h2>

<pre>
class input_format {
 public:
  virtual ~input_format() {}

  virtual bool <A HREF="#input_format_read_block">read_block</A>(istream&amp; is, string&amp; block) const = 0;
  virtual void <A HREF="#input_format_reset_document">reset_document</A>(<A HREF="#string_piece">string_piece</A> id = string_piece()) = 0;
  virtual void <A HREF="#input_format_set_text">set_text</A>(<A HREF="#string_piece">string_piece</A> text, bool make_copy = false) = 0;
  virtual bool <A HREF="#input_format_next_sentence">next_sentence</A>(<A HREF="#sentence">sentence</A>&amp; s, string&amp; error) = 0;

  // Static factory methods
  static <A HREF="#input_format">input_format</A>* <A HREF="#input_format_new_input_format">new_input_format</A>(const string&amp; name);
  static <A HREF="#input_format">input_format</A>* <A HREF="#input_format_new_conllu_input_format">new_conllu_input_format</A>(const string&amp; options = std::string());
  static <A HREF="#input_format">input_format</A>* <A HREF="#input_format_new_generic_tokenizer_input_format">new_generic_tokenizer_input_format</A>(const string&amp; options = std::string());
  static <A HREF="#input_format">input_format</A>* <A HREF="#input_format_new_horizontal_input_format">new_horizontal_input_format</A>(const string&amp; options = std::string());
  static <A HREF="#input_format">input_format</A>* <A HREF="#input_format_new_vertical_input_format">new_vertical_input_format</A>(const string&amp; options = std::string());

  static <A HREF="#input_format">input_format</A>* <A HREF="#input_format_new_presegmented_tokenizer">new_presegmented_tokenizer</A>(<A HREF="#input_format">input_format</A>* tokenizer);

  static const string CONLLU_V1;
  static const string CONLLU_V2;
  static const string GENERIC_TOKENIZER_NORMALIZED_SPACES;
  static const string GENERIC_TOKENIZER_PRESEGMENTED;
  static const string GENERIC_TOKENIZER_RANGES;
};
</pre>

<p>
The <a href="#input_format"><code>input_format</code></a> class allows loading sentences in various formats.
</p>
<p>
Th class instances may store internal state and are not thread-safe.
</p>

<a id="input_format_read_block" name="input_format_read_block"></a>
<h3>6.8.1. input_format::read_block()</h3>

<pre>
virtual bool read_block(istream&amp; is, string&amp; block) const = 0;
</pre>

<p>
Read a portion of input, which is guaranteed to contain only complete
sentences. Such portion is usually a paragraph (text followed by an empty line)
or a line, but it may be more complex (i.e., in a XML-like format).
</p>

<a id="input_format_reset_document" name="input_format_reset_document"></a>
<h3>6.8.2. input_format::reset_document()</h3>

<pre>
virtual void reset_document(<A HREF="#string_piece">string_piece</A> id = string_piece()) = 0;
</pre>

<p>
Resets the <a href="#input_format"><code>input_format</code></a> instance state. Such state
is needed not only for remembering unprocessed text of the last
<a href="#input_format_set_text"><code>set_text</code></a> call, but also for correct inter-block
state tracking (for example to track document-level ranges or inter-sentence spaces
-- if you pass only spaces to <a href="#input_format_set_text"><code>set_text</code></a>, these
spaces has to accumulate and be returned as preceding spaces of the next
sentence).
</p>
<p>
If applicable, first read sentence will have the <code># newdoc</code> comment, optionally
with given document id.
</p>

<a id="input_format_set_text" name="input_format_set_text"></a>
<h3>6.8.3. input_format::set_text()</h3>

<pre>
virtual void set_text(<A HREF="#string_piece">string_piece</A> text, bool make_copy = false) = 0;
</pre>

<p>
Set the text from which the sentences will be read.
</p>
<p>
If <code>make_copy</code> is <code>false</code>, only a reference to the given text is
stored and the user has to make sure it exists until the instance
is destroyed or <code>set_text</code> is called again. If <code>make_copy</code>
is <code>true</code>, a copy of the given text is made and retained until the
instance is destroyed or <code>set_text</code> is called again.
</p>

<a id="input_format_next_sentence" name="input_format_next_sentence"></a>
<h3>6.8.4. input_format::next_sentence()</h3>

<pre>
virtual bool next_sentence(<A HREF="#sentence">sentence</A>&amp; s, string&amp; error) = 0;
</pre>

<p>
Try reading another sentence from the text specified by
<a href="#input_format_set_text"><code>set_text</code></a>. Returns <code>true</code> if the sentence was
read and <code>false</code> if the text ended or there was a read error. The latter
two conditions can be distinguished by the <code>error</code> parameter &ndash; if it is
empty, the text ended, if it is nonempty, it contains a description of the
read error.
</p>

<a id="input_format_new_input_format" name="input_format_new_input_format"></a>
<h3>6.8.5. input_format::new_input_format()</h3>

<pre>
static <A HREF="#input_format">input_format</A>* new_input_format(const string&amp; name);
</pre>

<p style="margin-bottom:0">
Create new <a href="#input_format"><code>input_format</code></a> instance, given its name.
The individual input formats can be parametrized by using <code>format=data</code>
syntax. The following input formats are currently supported:
</p>
<ul style="margin-top:0">
<li><code>conllu</code>: return the <a href="#input_format_new_conllu_input_format"><code>new_conllu_input_format</code></a>
</li>
<li><code>generic_tokenizer</code>: return the <a href="#input_format_new_generic_tokenizer_input_format"><code>new_generic_tokenizer_input_format</code></a>
</li>
<li><code>horizontal</code>: return the <a href="#input_format_new_horizontal_input_format"><code>new_horizontal_input_format</code></a>
</li>
<li><code>vertical</code>: return the <a href="#input_format_new_vertical_input_format"><code>new_vertical_input_format</code></a>
</li>
</ul>

<p>
The new instance must be deleted after use.
</p>

<a id="input_format_new_conllu_input_format" name="input_format_new_conllu_input_format"></a>
<h3>6.8.6. input_format::new_conllu_input_format()</h3>

<pre>
static <A HREF="#input_format">input_format</A>* new_conllu_input_format(const string() options = std::string());
</pre>

<p>
Create <a href="#input_format"><code>input_format</code></a> instance which loads sentences
in the <a href="http://universaldependencies.github.io/docs/format.html">CoNLL-U format</a>.
The new instance must be deleted after use.
</p>
<p style="margin-bottom:0">
Supported options:
</p>
<ul style="margin-top:0">
<li><code>v2</code> (default): use CoNLL-U v2
</li>
<li><code>v1</code>: allow loading only CoNLL-U v1 (i.e., no empty nodes and no spaces in forms and lemmas)
</li>
</ul>

<a id="input_format_new_generic_tokenizer_input_format" name="input_format_new_generic_tokenizer_input_format"></a>
<h3>6.8.7. input_format::new_generic_tokenizer_input_format()</h3>

<pre>
static <A HREF="#input_format">input_format</A>* new_generic_tokenizer_input_format(const string() options = std::string());
</pre>

<p>
Create rule-based generic tokenizer for English-like languages (with spaces
separating tokens and English-like punctuation). The new instance must be
deleted after use.
</p>
<p style="margin-bottom:0">
Supported options:
</p>
<ul style="margin-top:0">
<li><code>normalized_spaces</code>: by default, UDPipe uses custom <code>misc</code> fields to exactly
  encode spaces in the original document. If <code>normalized_spaces</code> option is
  given, only standard CoNLL-U v2 markup (<code>SpaceAfter=No</code> and <code># newpar</code>)
  is used.
</li>
<li><code>presegmented</code>: input is assumed to be already segmented, with every
  sentence on a line, and is only tokenized (respecting sentence breaks)
</li>
<li><code>ranges</code>: for every token, range in the original document is stored in
  a format described in <a href="#token"><code>token</code></a> class
</li>
</ul>

<a id="input_format_new_horizontal_input_format" name="input_format_new_horizontal_input_format"></a>
<h3>6.8.8. input_format::new_horizontal_input_format()</h3>

<pre>
static <A HREF="#input_format">input_format</A>* new_horizontal_input_format(const string() options = std::string());
</pre>

<p>
Create <a href="#input_format"><code>input_format</code></a> instance which loads forms from a simple
horizontal format &ndash; each sentence on a line, with word forms separated by spaces.
The new instance must be deleted after use.
</p>
<p>
In order to allow spaces in tokens, Unicode character 'NO-BREAK SPACE' (U+00A0)
is considered part of token and converted to a space during loading.
</p>

<a id="input_format_new_vertical_input_format" name="input_format_new_vertical_input_format"></a>
<h3>6.8.9. input_format::new_vertical_input_format()</h3>

<pre>
static <A HREF="#input_format">input_format</A>* new_vertical_input_format(const string() options = std::string());
</pre>

<p>
Create <a href="#input_format"><code>input_format</code></a> instance which loads forms from a simple
vertical format &ndash; each word on a line, with empty line denoting end of sentence.
The new instance must be deleted after use.
</p>

<a id="input_format_new_presegmented_tokenizer" name="input_format_new_presegmented_tokenizer"></a>
<h3>6.8.10. input_format::new_presegmented_tokenizer()</h3>

<pre>
static <A HREF="#input_format">input_format</A>* new_presegmented_tokenizer(<A HREF="#input_format">input_format</A>* tokenizer);
</pre>

<p>
Create <a href="#input_format"><code>input_format</code></a> instance which acts as a tokenizer
adapter &ndash; given a tokenizer which segments anywhere, it creates a tokenizer
which segments on newline characters (by calling the tokenizer on individual lines,
and if the tokenizer segments in the middle of the line, it calls it repeatedly
and merges the results).
</p>
<p>
The new instance must be deleted after use. Note that the new instance
<i>takes ownership</i> of the given <code>tokenizer</code> and <i>deletes</i> it during
its own deletion.
</p>

<a id="output_format" name="output_format"></a>
<h2>6.9. Class output_format</h2>

<pre>
class output_format {
 public:
  virtual ~output_format() {}

  virtual void <A HREF="#output_format_write_sentence">write_sentence</A>(const <A HREF="#sentence">sentence</A>&amp; s, ostream&amp; os) = 0;
  virtual void <A HREF="#output_format_finish_document">finish_document</A>(ostream&amp; os) {};

  // Static factory methods
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_output_format">new_output_format</A>(const string&amp; name);
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_conllu_output_format">new_conllu_output_format</A>(const string() options = std::string());
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_epe_output_format">new_epe_output_format</A>(const string() options = std::string());
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_matxin_output_format">new_matxin_output_format</A>(const string() options = std::string());
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_horizontal_output_format">new_horizontal_output_format</A>(const string() options = std::string());
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_plaintext_output_format">new_plaintext_output_format</A>(const string() options = std::string());
  static <A HREF="#output_format">output_format</A>* <A HREF="#output_format_new_vertical_output_format">new_vertical_output_format</A>(const string() options = std::string());

  static const string CONLLU_V1;
  static const string CONLLU_V2;
  static const string HORIZONTAL_PARAGRAPHS;
  static const string PLAINTEXT_NORMALIZED_SPACES;
  static const string VERTICAL_PARAGRAPHS;
};
</pre>

<p>
The <a href="#output_format"><code>output_format</code></a> class allows printing sentences
in various formats.
</p>
<p>
The class instances may store internal state and are not thread-safe.
</p>

<a id="output_format_write_sentence" name="output_format_write_sentence"></a>
<h3>6.9.1. output_format::write_sentence()</h3>

<pre>
virtual void write_sentence(const <A HREF="#sentence">sentence</A>&amp; s, ostream&amp; os) = 0;
</pre>

<p>
Write given <a href="#sentence"><code>sentence</code></a> to the given output stream.
</p>
<p>
When the output format requires document-level markup, it is written
automatically when the first sentence is written using this
<a href="#output_format"><code>output_format</code></a> instance (or after
<a href="#output_format_finish_document"><code>finish_document</code></a> call).
</p>

<a id="output_format_finish_document" name="output_format_finish_document"></a>
<h3>6.9.2. output_format::finish_document()</h3>

<pre>
virtual void finish_document(ostream&amp; os) {};
</pre>

<p>
When the output format requires document-level markup, write
the end-of-document mark and reset the <a href="#output_format"><code>output_format</code></a>
instance state (i.e., the next <a href="#write_sentence"><code>write_sentence</code></a>
will write start-of-document mark).
</p>

<a id="output_format_new_output_format" name="output_format_new_output_format"></a>
<h3>6.9.3. output_format::new_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_output_format(const string&amp; name);
</pre>

<p style="margin-bottom:0">
Create new <a href="#output_format"><code>output_format</code></a> instance, given its name.
The following output formats are currently supported:
</p>
<ul style="margin-top:0">
<li><code>conllu</code>: return the <a href="#output_format_new_conllu_output_format"><code>new_conllu_output_format</code></a>
</li>
<li><code>epe</code>: return the <a href="#output_format_new_epe_output_format"><code>new_epe_output_format</code></a>
</li>
<li><code>matxin</code>: return the <a href="#output_format_new_matxin_output_format"><code>new_matxin_output_format</code></a>
</li>
<li><code>horizontal</code>: return the <a href="#output_format_new_horizontal_output_format"><code>new_horizontal_output_format</code></a>
</li>
<li><code>plaintext</code>: return the <a href="#output_format_new_plaintext_output_format"><code>new_plaintext_output_format</code></a>
</li>
<li><code>vertical</code>: return the <a href="#output_format_new_vertical_output_format"><code>new_vertical_output_format</code></a>
</li>
</ul>

<p>
The new instance must be deleted after use.
</p>

<a id="output_format_new_conllu_output_format" name="output_format_new_conllu_output_format"></a>
<h3>6.9.4. output_format::new_conllu_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_conllu_output_format(const string() options = std::string());
</pre>

<p>
Creates <a href="#output_format"><code>output_format</code></a> instance for writing sentences
in the <a href="http://universaldependencies.github.io/docs/format.html">CoNLL-U format</a>.
The new instance must be deleted after use.
</p>
<p style="margin-bottom:0">
Supported options:
</p>
<ul style="margin-top:0">
<li><code>v2</code> (default): use CoNLL-U v2
</li>
<li><code>v1</code>: produce output in CoNLL-U v1 format. Note that this is a lossy process, as
empty nodes are ignored and spaces in forms and lemmas are converted to underscores.
</li>
</ul>

<a id="output_format_new_epe_output_format" name="output_format_new_epe_output_format"></a>
<h3>6.9.5. output_format::new_epe_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_epe_output_format(const string() options = std::string());
</pre>

<p>
Creates <a href="#output_format"><code>output_format</code></a> instance for writing sentences
in the EPE (Extrinsic Parser Evaluation 2017) interchange format.
The new instance must be deleted after use.
</p>

<a id="output_format_new_matxin_output_format" name="output_format_new_matxin_output_format"></a>
<h3>6.9.6. output_format::new_matxin_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_matxin_output_format(const string() options = std::string());
</pre>

<p style="margin-bottom:0">
Creates <a href="#output_format"><code>output_format</code></a> instance for writing sentences
in the Matxin format &ndash; UDPipe produces a XML with the following DTD:
</p>
<pre style="margin-top:0">
&lt;!ELEMENT    corpus     (SENTENCE*)&gt;
&lt;!ELEMENT    SENTENCE   (NODE*)&gt;
&lt;!ATTLIST    SENTENCE    ord           CDATA        #REQUIRED
                         alloc         CDATA        #REQUIRED&gt;
&lt;!ELEMENT    NODE   (NODE*)&gt;
&lt;!ATTLIST    NODE        ord           CDATA        #REQUIRED
                         alloc         CDATA        #REQUIRED
                         form          CDATA        #REQUIRED
                         lem           CDATA        #REQUIRED
                         mi            CDATA        #REQUIRED
                         si            CDATA        #REQUIRED
                         sub           CDATA        #REQUIRED&gt;
</pre>

<p>
The new instance must be deleted after use.
</p>

<a id="output_format_new_plaintext_output_format" name="output_format_new_plaintext_output_format"></a>
<h3>6.9.7. output_format::new_plaintext_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_plaintext_output_format(const string() options = std::string());
</pre>

<p style="margin-bottom:0">
Creates <a href="#output_format"><code>output_format</code></a> instance for writing sentence
<i>tokens</i> (in the UD sense) using original spacing.
By default, UDPipe custom <code>misc</code> features (see description of
<a href="#token"><code>token</code></a> class) are used to reconstruct the exact original spaces.
However, if the document does not contain these features or if only
normalized spacing is wanted, you can use the following option:
</p>
<ul style="margin-top:0">
<li><code>normalized_spaces</code>: write one sentence on a line, and either one or no space between
  tokens, using the <code>SpaceAfter=No</code> feature
</li>
</ul>

<a id="output_format_new_horizontal_output_format" name="output_format_new_horizontal_output_format"></a>
<h3>6.9.8. output_format::new_horizontal_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_horizontal_output_format(const string() options = std::string());
</pre>

<p>
Creates <a href="#output_format"><code>output_format</code></a> instance for writing sentences
in a simple horizontal format &ndash; each sentence on a line, with word forms separated
by spaces. The new instance must be deleted after use.
</p>
<p>
Because words can contain spaces in CoNLL-U v2, the spaces in words are
converted to Unicode character 'NO-BREAK SPACE' (U+00A0).
</p>
<p style="margin-bottom:0">
Supported options:
</p>
<ul style="margin-top:0">
<li><code>paragraphs</code>: if given, an empty line is printed after the end of a paragraph
or a document (recognized by <code># newpar</code> or <code># newdoc</code> comments)
</li>
</ul>

<a id="output_format_new_vertical_output_format" name="output_format_new_vertical_output_format"></a>
<h3>6.9.9. output_format::new_vertical_output_format()</h3>

<pre>
static <A HREF="#output_format">output_format</A>* new_vertical_output_format(const string() options = std::string());
</pre>

<p>
Creates <a href="#output_format"><code>output_format</code></a> instance for writing sentences
in a simple vertical format &ndash; each word form on a line, with empty line
denoting end of sentence. The new instance must be deleted after use.
</p>
<p style="margin-bottom:0">
Supported options:
</p>
<ul style="margin-top:0">
<li><code>paragraphs</code>: if given, an empty line is printed after the end of a paragraph
or a document (recognized by <code># newpar</code> or <code># newdoc</code> comments)
</li>
</ul>

<a id="model" name="model"></a>
<h2>6.10. Class model</h2>

<pre>
class model {
 public:
  virtual ~model() {}

  static <A HREF="#model">model</A>* <A HREF="#model_load_cstring">load</A>(const char* fname);
  static <A HREF="#model">model</A>* <A HREF="#model_load_istream">load</A>(istream&amp; is);

  virtual <A HREF="#input_format">input_format</A>* <A HREF="#model_new_tokenizer">new_tokenizer</A>(const string&amp; options) const = 0;
  virtual bool <A HREF="#model_tag">tag</A>(<A HREF="#sentence">sentence</A>&amp; s, const string&amp; options, string&amp; error) const = 0;
  virtual bool <A HREF="#model_parse">parse</A>(<A HREF="#sentence">sentence</A>&amp; s, const string&amp; options, string&amp; error) const = 0;

  static const string DEFAULT;
  static const string TOKENIZER_NORMALIZED_SPACES;
  static const string TOKENIZER_PRESEGMENTED;
  static const string TOKENIZER_RANGES;
};
</pre>

<p>
Class representing UDPipe model, allowing to perform tokenization,
tagging and parsing.
</p>

<a id="model_load_cstring" name="model_load_cstring"></a>
<h3>6.10.1. model::load(const char*)</h3>

<pre>
static <A HREF="#model">model</A>* load(const char* fname);
</pre>

<p>
Load a new model from a given file, returning <code>NULL</code> on failure.
The new instance must be deleted after use.
</p>

<a id="model_load_istream" name="model_load_istream"></a>
<h3>6.10.2. model::load(istream&amp;)</h3>

<pre>
static <A HREF="#model">model</A>* load(istream&amp; is);
</pre>

<p>
Load a new model from a given input stream, returning <code>NULL</code> on failure.
The new instance must be deleted after use.
</p>

<a id="model_new_tokenizer" name="model_new_tokenizer"></a>
<h3>6.10.3. model::new_tokenizer()</h3>

<pre>
virtual <A HREF="#input_format">input_format</A>* new_tokenizer(const string&amp; options) const = 0;
</pre>

<p>
Construct a new tokenizer (or <code>NULL</code> if no tokenizer is specified by the model).
The new instance must be deleted after use.
</p>

<a id="model_tag" name="model_tag"></a>
<h3>6.10.4. model::tag()</h3>

<pre>
virtual bool tag(<A HREF="#sentence">sentence</A>&amp; s, const string&amp; options, string&amp; error) const = 0;
</pre>

<p>
Tag the given sentence.
</p>

<a id="model_parse" name="model_parse"></a>
<h3>6.10.5. model::parse()</h3>

<pre>
virtual bool parse(<A HREF="#sentence">sentence</A>&amp; s, const string&amp; options, string&amp; error) const = 0;
</pre>

<p>
Parse the given sentence.
</p>

<a id="pipeline" name="pipeline"></a>
<h2>6.11. Class pipeline</h2>

<pre>
class pipeline {
 public:
  pipeline(const <A HREF="#model">model</A>* m, const string&amp; input, const string&amp; tagger, const string&amp; parser, const string&amp; output);

  void <A HREF="#pipeline_set_model">set_model</A>(const <A HREF="#model">model</A>* m);
  void <A HREF="#pipeline_set_input">set_input</A>(const string&amp; input);
  void <A HREF="#pipeline_set_tagger">set_tagger</A>(const string&amp; tagger);
  void <A HREF="#pipeline_set_parser">set_parser</A>(const string&amp; parser);
  void <A HREF="#pipeline_set_output">set_output</A>(const string&amp; output);

  void <A HREF="#pipeline_set_immediate">set_immediate</A>(bool immediate);
  void [set_document_id #pipeline_set_document_id[(const string&amp; document_id);

  bool <A HREF="#pipeline_process">process</A>(istream&amp; is, ostream&amp; os, string&amp; error) const;

  static const string DEFAULT;
  static const string NONE;
};
</pre>

<p>
The <a href="#pipeline"><code>pipeline</code></a> class allows simple file-to-file processing.
A model and input/tagger/parser/output options can be specified in the pipeline.
</p>
<p>
The input file can be processed either after fully loaded (default),
or in immediate mode, in which case is the input processed and printed as soon
as a block of input guaranteed to contain whole sentences is loaded.
Specifically, for most input formats the input is processed after loading an
empty line (with the exception of <code>horizontal</code> input format and
<code>presegmented</code> tokenizer, where the input is processed after loading every
line).
</p>

<a id="pipeline_set_model" name="pipeline_set_model"></a>
<h3>6.11.1. pipeline::set_model()</h3>

<pre>
void set_model(const <A HREF="#model">model</A>* m);
</pre>

<p>
Use the given model.
</p>

<a id="pipeline_set_input" name="pipeline_set_input"></a>
<h3>6.11.2. pipeline::set_input()</h3>

<pre>
void set_input(const string&amp; input);
</pre>

<p>
Use the given input format. In addition to formats described in
<a href="#input_format_new_input_format"><code>new_input_format</code></a>, a special
<code>tokenizer</code> or <code>tokenizer=options</code> format allows using the
model tokenizer.
</p>

<a id="pipeline_set_tagger" name="pipeline_set_tagger"></a>
<h3>6.11.3. pipeline::set_tagger()</h3>

<pre>
void set_tagger(const string&amp; tagger);
</pre>

<p>
Use the given tagger options.
</p>

<a id="pipeline_set_parser" name="pipeline_set_parser"></a>
<h3>6.11.4. pipeline::set_parser()</h3>

<pre>
void set_parser(const string&amp; parser);
</pre>

<p>
Use the given parser options.
</p>

<a id="pipeline_set_output" name="pipeline_set_output"></a>
<h3>6.11.5. pipeline::set_output()</h3>

<pre>
void set_output(const string&amp; output);
</pre>

<p>
Use the given output format (see
<a href="#output_format_new_output_format"><code>new_output_format</code></a> for a list).
</p>

<a id="pipeline_set_immediate" name="pipeline_set_immediate"></a>
<h3>6.11.6. pipeline::set_immediate()</h3>

<pre>
void set_immediate(bool immediate);
</pre>

<p>
Set or reset the immediate mode (default is <code>immediate=false</code>).
</p>

<a id="pipeline_set_document_id" name="pipeline_set_document_id"></a>
<h3>6.11.7. pipeline::set_document_id()</h3>

<pre>
void set_document_id(const string&amp; document_id);
</pre>

<p>
Set document id, which is passed to
<a href="#input_format_reset_document"><code>input_format::reset_document</code></a>).
</p>

<a id="pipeline_process" name="pipeline_process"></a>
<h3>6.11.8. pipeline::process()</h3>

<pre>
bool process(istream&amp; is, ostream&amp; os, string&amp; error) const;
</pre>

<p>
Process the given input stream, writing results to the given output stream.
If the processing succeeded, <code>true</code> is returned; otherwise, <code>false</code>
is returned with an error stored in the <code>error</code> argument.
</p>

<a id="trainer" name="trainer"></a>
<h2>6.12. Class trainer</h2>

<pre>
class trainer {
 public:
  static bool <A HREF="#trainer_train">train</A>(const string&amp; method, const vector&lt;<A HREF="#sentence">sentence</A>&gt;&amp; train, const vector&lt;<A HREF="#sentence">sentence</A>&gt;&amp; heldout,
                    const string&amp; tokenizer, const string&amp; tagger, const string&amp; parser,
                    ostream&amp; os, string&amp; error);

  static const string DEFAULT;
  static const string NONE;
};
</pre>

<p>
Class allowing training a UDPipe model.
</p>

<a id="trainer_train" name="trainer_train"></a>
<h3>6.12.1. trainer::train()</h3>

<pre>
static bool train(const string&amp; method, const vector&lt;<A HREF="#sentence">sentence</A>&gt;&amp; train, const vector&lt;<A HREF="#sentence">sentence</A>&gt;&amp; heldout,
                  const string&amp; tokenizer, const string&amp; tagger, const string&amp; parser,
                  ostream&amp; os, string&amp; error);
</pre>

<p>
Train a UDPipe model. The only supported method is currently <code>morphodita_parsito</code>.
Use the supplied train and heldout data, and given tokenizer, tagger and parser
options (see the Training UDPipe Models section in the User's Manual).
</p>
<p>
If the training succeeded, <code>true</code> is returned and the model is saved to the
given <code>os</code> stream; otherwise, <code>false</code> is returned with an error stored in
the <code>error</code> argument.
</p>

<a id="evaluator" name="evaluator"></a>
<h2>6.13. Class evaluator</h2>

<pre>
class evaluator {
 public:
  evaluator(const <A HREF="#model">model</A>* m, const string&amp; tokenizer, const string&amp; tagger, const string&amp; parser);

  void <A HREF="#evaluator_set_model">set_model</A>(const <A HREF="#model">model</A>* m);
  void <A HREF="#evaluator_set_tokenizer">set_tokenizer</A>(const string&amp; tokenizer);
  void <A HREF="#evaluator_set_tagger">set_tagger</A>(const string&amp; tagger);
  void <A HREF="#evaluator_set_parser">set_parser</A>(const string&amp; parser);

  bool <A HREF="#evaluator_evaluate">evaluate</A>(istream&amp; is, ostream&amp; os, string&amp; error) const;

  static const string DEFAULT;
  static const string NONE;
};
</pre>

<p>
Class evaluating performance of given model on CoNLL-U file.
</p>
<p>
Three different settings (depending on whether tokenizer, tagger and parser is used)
can be evaluated. For details, see Measuring Model Accuracy in User's Manual.
</p>

<a id="evaluator_set_model" name="evaluator_set_model"></a>
<h3>6.13.1. evaluator::set_model()</h3>

<pre>
void set_model(const <A HREF="#model">model</A>* m);
</pre>

<p>
Use the given model.
</p>

<a id="evaluator_set_tokenizer" name="evaluator_set_tokenizer"></a>
<h3>6.13.2. evaluator::set_tokenizer()</h3>

<pre>
void set_tokenizer(const string&amp; tokenizer);
</pre>

<p>
Use the given tokenizer options; pass <code>DEFAULT</code> to use default
options or <code>NONE</code> not to use a tokenizer.
</p>

<a id="evaluator_set_tagger" name="evaluator_set_tagger"></a>
<h3>6.13.3. evaluator::set_tagger()</h3>

<pre>
void set_tagger(const string&amp; tagger);
</pre>

<p>
Use the given tagger options; pass <code>DEFAULT</code> to use default
options or <code>NONE</code> not to use a tagger.
</p>

<a id="evaluator_set_parser" name="evaluator_set_parser"></a>
<h3>6.13.4. evaluator::set_parser()</h3>

<pre>
void set_parser(const string&amp; parser);
</pre>

<p>
Use the given parser options; pass <code>DEFAULT</code> to use default
options or <code>NONE</code> not to use a parser.
</p>

<a id="evaluator_evaluate" name="evaluator_evaluate"></a>
<h3>6.13.5. evaluator::evaluate()</h3>

<pre>
bool evaluate(istream&amp; is, ostream&amp; os, string&amp; error) const;
</pre>

<p>
Evaluate the specified model on the given CoNLL-U input read
from <code>is</code> stream.
</p>
<p>
If the evaluation succeeded, <code>true</code> is returned and the evaluation
results are written to the <code>os</code> stream in a plain text format;
otherwise, <code>false</code> is returned with an error stored in
the <code>error</code> argument.
</p>

<a id="version" name="version"></a>
<h2>6.14. Class version</h2>

<pre>
class version {
 public:
  unsigned major;
  unsigned minor;
  unsigned patch;
  string prerelease;

  static <A HREF="#version">version</A> <A HREF="#version_current">current</A>();
};
</pre>

<p>
The <a href="#version"><code>version</code></a> class represents UDPipe version.
See <a href="#versioning">UDPipe Versioning</a> for more information.
</p>

<a id="version_current" name="version_current"></a>
<h3>6.14.1. version::current</h3>

<pre>
static <A HREF="#version">version</A> current();
</pre>

<p>
Returns current UDPipe version.
</p>

<a id="cpp_bindings_api" name="cpp_bindings_api"></a>
<h2>6.15. C++ Bindings API</h2>

<p>
Bindings for other languages than C++ are created using SWIG from the C++
bindings API, which is a slightly modified version of the native C++ API.
Main changes are replacement of <a href="#string_piece"><code>string_piece</code></a> type by native
strings and removal of methods using <code>istream</code>. Here is the C++ bindings API
declaration:
</p>

<a id="bindings_helper_structures" name="bindings_helper_structures"></a>
<h3>6.15.1. Helper Structures</h3>

<pre>
typedef vector&lt;int&gt; Children;

typedef vector&lt;string&gt; Comments;

class ProcessingError {
public:
  bool occurred();
  string message;
};

class Token {
 public:
  string form;
  string misc;

  Token(const string&amp; form = string(), const string&amp; misc = string());

  // CoNLL-U defined SpaceAfter=No feature
  bool getSpaceAfter() const;
  void setSpaceAfter(bool space_after);

  // UDPipe-specific all-spaces-preserving SpacesBefore and SpacesAfter features
  string getSpacesBefore() const;
  void setSpacesBefore(const string&amp; spaces_before);
  string getSpacesAfter() const;
  void setSpacesAfter(const string&amp; spaces_after);
  string getSpacesInToken() const;
  void setSpacesInToken(const string&amp; spaces_in_token);

  // UDPipe-specific TokenRange feature
  bool getTokenRange() const;
  size_t getTokenRangeStart() const;
  size_t getTokenRangeEnd() const;
  void setTokenRange(size_t start, size_t end);
};

class Word : public Token {
 public:
  // form and misc are inherited from token
  int id;         // 0 is root, &gt;0 is sentence word, &lt;0 is undefined
  string lemma;   // lemma
  string upostag; // universal part-of-speech tag
  string xpostag; // language-specific part-of-speech tag
  string feats;   // list of morphological features
  int head;       // head, 0 is root, &lt;0 is undefined
  string deprel;  // dependency relation to the head
  string deps;    // secondary dependencies

  Children children;

  Word(int id = -1, const string&amp; form = string());
};
typedef vector&lt;Word&gt; Words;

class MultiwordToken : public Token {
 public:
  // form and misc are inherited from token
  int idFirst, idLast;

  MultiwordToken(int id_first = -1, int id_last = -1, const string&amp; form = string(), const string&amp; misc = string());
};
typedef vector&lt;MultiwordToken&gt; MultiwordTokens;

class EmptyNode {
 public:
  int id;          // 0 is root, &gt;0 is sentence word, &lt;0 is undefined
  int index;       // index for the current id, should be numbered from 1, 0=undefined
  string form;     // form
  string lemma;    // lemma
  string upostag;  // universal part-of-speech tag
  string xpostag;  // language-specific part-of-speech tag
  string feats;    // list of morphological features
  string deps;     // secondary dependencies
  string misc;     // miscellaneous information

  EmptyNode(int id = -1, int index = 0) : id(id), index(index) {}
};
typedef vector&lt;empty_node&gt; EmptyNodes;

class Sentence {
 public:
  Sentence();

  Words words;
  MultiwordTokens multiwordTokens;
  EmptyNodes emptyNodes;
  Comments comments;
  static const string rootForm;

  // Basic sentence modifications
  bool empty();
  void clear();
  virtual Word&amp; addWord(const char* form);
  void setHead(int id, int head, const string&amp; deprel);
  void unlinkAllWords();

  // CoNLL-U defined comments
  bool getNewDoc() const;
  string getNewDocId() const;
  void setNewDoc(bool new_doc, const string&amp; id = string());
  bool getNewPar() const;
  string getNewParId() const;
  void setNewPar(bool new_par, const string&amp; id = string());

  string getSentId() const;
  void setSentId(const string&amp; id);
  string getText() const;
  void setText(const string&amp; id);
};
typedef vector&lt;Sentence&gt; Sentences;
</pre>

<a id="bindings_main_classes" name="bindings_main_classes"></a>
<h3>6.15.2. Main Classes</h3>

<pre>
class InputFormat {
 public:
  virtual void resetDocument(const string&amp; id = string());
  virtual void setText(const char* text);
  virtual bool nextSentence(Sentence&amp; s, ProcessingError* error = nullptr);

  static InputFormat* newInputFormat(const string&amp; name);
  static InputFormat* newConlluInputFormat(const string&amp; id = string());
  static InputFormat* newGenericTokenizerInputFormat(const string&amp; id = string());
  static InputFormat* newHorizontalInputFormat(const string&amp; id = string());
  static InputFormat* newVerticalInputFormat(const string&amp; id = string());

  static InputFormat* newPresegmentedTokenizer(InputFormat tokenizer);

  static const string CONLLU_V1;
  static const string CONLLU_V2;
  static const string GENERIC_TOKENIZER_NORMALIZED_SPACES;
  static const string GENERIC_TOKENIZER_PRESEGMENTED;
  static const string GENERIC_TOKENIZER_RANGES;
};

class OutputFormat {
 public:
  virtual string writeSentence(const Sentence&amp; s);
  virtual string finishDocument();

  static OutputFormat* newOutputFormat(const string&amp; name);
  static OutputFormat* newConlluOutputFormat(const string&amp; options = string());
  static OutputFormat* newEpeOutputFormat(const string&amp; options = string());
  static OutputFormat* newMatxinOutputFormat(const string&amp; options = string());
  static OutputFormat* newHorizontalOutputFormat(const string&amp; options = string());
  static OutputFormat* newPlaintextOutputFormat(const string&amp; options = string());
  static OutputFormat* newVerticalOutputFormat(const string&amp; options = string());

  static const string CONLLU_V1;
  static const string CONLLU_V2;
  static const string HORIZONTAL_PARAGRAPHS;
  static const string PLAINTEXT_NORMALIZED_SPACES;
  static const string VERTICAL_PARAGRAPHS;
};

class Model {
 public:
  static Model* load(const char* fname);

  virtual InputFormat* newTokenizer(const string&amp; options) const;
  virtual bool tag(Sentence&amp; s, const string&amp; options, ProcessingError* error = nullptr) const;
  virtual bool parse(Sentence&amp; s, const string&amp; options, ProcessingError* error) const;

  static const string DEFAULT;
  static const string TOKENIZER_PRESEGMENTED;
};

class Pipeline {
 public:
  Pipeline(const Model* m, const string&amp; input, const string&amp; tagger, const string&amp; parser, const string&amp; output);

  void setModel(const Model* m);
  void setInput(const string&amp; input);
  void setTagger(const string&amp; tagger);
  void setParser(const string&amp; parser);
  void setOutput(const string&amp; output);

  void setImmediate(bool immediate);
  void setDocumentId(const string&amp; document_id);

  string process(const string&amp; data, ProcessingError* error = nullptr) const;

  static const string DEFAULT;
  static const string NONE;
};

class Trainer {
 public:

  static string train(const string&amp; method, const Sentences&amp; train, const Sentences&amp; heldout,
                      const string&amp; tokenizer, const string&amp; tagger, const string&amp; parser,
                      ProcessingError* error = nullptr);

  static const string DEFAULT;
  static const string NONE;
};

class Evaluator {
 public:
  Evaluator(const Model* m, const string&amp; tokenizer, const string&amp; tagger, const string&amp; parser);

  void setModel(const Model* m);
  void setTokenizer(const string&amp; tokenizer);
  void setTagger(const string&amp; tagger);
  void setParser(const string&amp; parser);

  string evaluate(const string&amp; data, ProcessingError* error = nullptr) const;

  static const string DEFAULT;
  static const string NONE;
};

class Version {
 public:
  unsigned major;
  unsigned minor;
  unsigned patch;
  string prerelease;

  // Returns current version.
  static version current();
};
</pre>

<a id="csharp_bindings" name="csharp_bindings"></a>
<h2>6.16. C# Bindings</h2>

<p>
UDPipe library bindings is available in the <code>Ufal.UDPipe</code> namespace.
</p>
<p>
The bindings is a straightforward conversion of the <code>C++</code> bindings API.
The bindings requires native C++ library <code>libudpipe_csharp</code> (called
<code>udpipe_csharp</code> on Windows).
</p>

<a id="java_bindings" name="java_bindings"></a>
<h2>6.17. Java Bindings</h2>

<p>
UDPipe library bindings is available in the <code>cz.cuni.mff.ufal.udpipe</code>
package.
</p>
<p>
The bindings is a straightforward conversion of the <code>C++</code> bindings API.
Vectors do not have native Java interface, see
<code>cz.cuni.mff.ufal.udpipe.Words</code> class for reference. Also, class members
are accessible and modifiable using using <code>getField</code> and <code>setField</code>
wrappers.
</p>
<p>
The bindings require native C++ library <code>libudpipe_java</code> (called
<code>udpipe_java</code> on Windows). If the library is found in the current
directory, it is used, otherwise standard library search process is used.
The path to the C++ library can also be specified using static
<code>udpipe_java.setLibraryPath(String path)</code> call (before the first call
inside the C++ library, of course).
</p>

<a id="perl_bindings" name="perl_bindings"></a>
<h2>6.18. Perl Bindings</h2>

<p>
UDPipe library bindings is available in the
<a href="http://search.cpan.org/~straka/Ufal-UDPipe/"><code>Ufal::UDPipe</code></a> package.
The classes can be imported into the current namespace using the <code>:all</code>
export tag.
</p>
<p>
The bindings is a straightforward conversion of the <code>C++</code> bindings API.
Vectors do not have native Perl interface, see <code>Ufal::UDPipe::Words</code> for
reference. Static methods and enumerations are available only through the
module, not through object instance.
</p>

<a id="python_bindings" name="python_bindings"></a>
<h2>6.19. Python Bindings</h2>

<p>
UDPipe library bindings is available in the
<a href="http://pypi.python.org/pypi/ufal.udpipe"><code>ufal.udpipe</code></a> module.
</p>
<p>
The bindings is a straightforward conversion of the <code>C++</code> bindings API.
In Python 2, strings can be both <code>unicode</code> and UTF-8 encoded <code>str</code>, and the
library always produces <code>unicode</code>. In Python 3, strings must be only <code>str</code>.
</p>

<a id="contact" name="contact"></a>
<h1>7. Contact</h1>

<p style="margin-bottom:0">
Authors:
</p>
<ul style="margin-top:0">
<li><a href="http://ufal.mff.cuni.cz/milan-straka">Milan Straka</a>, <a href="mailto:straka@ufal.mff.cuni.cz">straka@ufal.mff.cuni.cz</a>
</li>
</ul>

<p>
<a href="http://ufal.mff.cuni.cz/udpipe">UDPipe website</a>.
</p>
<p>
<a href="http://hdl.handle.net/11234/1-1702">UDPipe LINDAT/CLARIN entry</a>.
</p>

<a id="udpipe_acknowledgements" name="udpipe_acknowledgements"></a>
<h1>8. Acknowledgements</h1>

<p>
This work has been using language resources developed and/or stored and/or distributed by the LINDAT/CLARIN project of the Ministry of Education of the Czech Republic (project <i>LM2010013</i>).
</p>
<p>
Acknowledgements for individual language models are listed in <a href="#users_manual">UDPipe User's Manual</a>.
</p>

<a id="publications" name="publications"></a>
<h2>8.1. Publications</h2>

<ul>
<li>(Straka et al. 2017) Milan Straka and Jana Strakov√°. <i><a href="http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf">Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe</a></i>. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Vancouver, Canada, August 2017.
</li>
<li>(Straka et al. 2016) Straka Milan, Hajiƒç Jan, Strakov√° Jana. <i><a href="http://ufal.mff.cuni.cz/~straka/papers/2016-lrec_udpipe.pdf">UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing</a></i>. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Portoro≈æ, Slovenia, May 2016.
</li>
</ul>

<a id="bibtex_for_referencing" name="bibtex_for_referencing"></a>
<h2>8.2. Bibtex for Referencing</h2>

<pre>
@InProceedings{udpipe:2017,
  author    = {Straka, Milan  and  Strakov\'{a}, Jana},
  title     = {Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe},
  booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {88--99},
  url       = {http://www.aclweb.org/anthology/K/K17/K17-3009.pdf}
}
</pre>

<a id="persistent_identifier" name="persistent_identifier"></a>
<h2>8.3. Persistent Identifier</h2>

<p>
If you prefer to reference UDPipe by a persistent identifier (PID),
you can use <code>http://hdl.handle.net/11234/1-1702</code>.
</p>
</div>

<!-- html code generated by txt2tags 2.6 (http://txt2tags.org) -->
<!-- cmdline: txt2tags -t html -\-toc -\-enum-title -o manual.html -C t2t_docsys/t2t_docsys_manual.conf -C t2t_align_percent_cells_right.conf manual.t2t -->
</body></html>
